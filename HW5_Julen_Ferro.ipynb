{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### USE GITHUB SOL, YUHANGZOU88, AND SOLUS LIBRO\n",
    "\n",
    "6.5, 6.7 no estan en solus libro\n",
    "\n",
    "7.4 y 7.5 SIIIIIIIIII"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1 Ex. 6.5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show that fitting a locally constant multinomial logit model of the form (6.19) amounts to smoothing the binary response indicators for each class separately using a Nadaraya-Watson kernel smoother with kernel weights $K_{\\lambda}(x_0,x_i)$.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we smooth binary response indicators for each class separately using a Nadaraya-Watson kernel smoother, recall equation (6.2) in the text, for any class $j\\in {1,...,K}$, $y_i=1$ if and only if $i\\in G_j$, where $G_j$ is the set of indices that belong to class $j$. Then we have:\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{P}(G=j|X=x_0) = \\frac{\\sum_{i\\in G_j} K_\\lambda(x_0, x_i)}{\\sum_{i=1}^N K_\\lambda(x_0, x_i)}\\propto \\sum_{i\\in G_j} K_\\lambda(x_0, x_i).\n",
    "\\end{equation}\n",
    "\n",
    "That means for $x_0$, we classify it to class $j$ that maximizes $\\sum_{i\\in G_j} K_\\lambda(x_0, x_i)$. On the other hand, consider the local multinomial logit model (6.19) in the text. From equation (6.20) in the text,\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\text{Pr}}(G=j|X=x_0) = \\frac{e^{\\hat\\beta_{j0}(x_0)}}{1+\\sum_{k=1}^{J-1}e^{\\hat\\beta_{k0}(x_0)}}.\n",
    "\\end{equation}\n",
    "\n",
    "We know that we classify $x_0$ to class $j$ that maximizes $\\hat\\beta_{j0}$ for each class $j$. It thus suffices to show that $\\hat\\beta_{j0}$ is a non-decreasing function of $\\sum_{i\\in G_j} K_\\lambda(x_0, x_i)$.\n",
    "\n",
    "Let $\\beta$ denote the parameter set ${\\beta_{k0}, \\beta_k, k=1,...,K-1}$. We code the class from $1$ to $K$ so that the log-likelihood $l(\\beta, x_0)$ can be written as:\n",
    "\n",
    "\\begin{equation}\n",
    "l(\\beta, x_0) &=\\sum_{i=1}^N K_\\lambda(x_0, x_i)\\Bigg{\\sum_{k=1}^{K-1}\\bb{1}(y_i=k)\\left[\\beta_{k0}(x_0) + \\beta_{k}(x_0)^T(x_i-x_0)\\right] \\nonumber \\\n",
    "&\\quad-\\log\\Big(1+\\sum_{l=1}^{K-1}e^{\\beta_{l0}(x_0) + \\beta_{l}^T(x_i-x_0)}\\Big)\\Bigg}.\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "To maximize the log-likelihood, we need to set its derivatives to zero and then solve the equations to find $\\hat\\beta_{k0}$. These equations, for $j=1,...,K-1$, are:\n",
    "\n",
    "\\begin{equation}\n",
    "\\label{eq:6-5b}\n",
    "\\frac{\\partial l(\\beta, x_0)}{\\partial \\beta_{j0}} &=& \\sum_{i=1}^N K_\\lambda(x_0, x_i) \\left(\\mathbb{1}(y_i=j) - \\frac{e^{\\beta_{j0} + \\beta_{j}^T(x_i-x_0)}}{1+\\sum_{l=1}^{K-1}e^{\\beta_{l0} + \\beta_l^T(x_i-x_0)}}\\right)\\nonumber \\\n",
    "&=& \\sum_{i\\in G_j} K_\\lambda(x_0, x_i) -e^{\\beta_{j0}}\\cdot\\sum_{i=1}^NK_\\lambda(x_0, x_i)\\frac{e^{\\beta_j^T(x_i-x_0)}}{1+\\sum_{l=1}^{K-1}e^{\\beta_{l0} + \\beta_l^T(x_i-x_0)}}\\nonumber \\\n",
    "&=&0.\\nonumber\n",
    "\\end{equation}\n",
    "\n",
    "Therefore we know that:\n",
    "\n",
    "\\begin{equation}\n",
    "\\exp(\\hat\\beta_{j0}) \\propto \\sum_{i\\in G_j} K_\\lambda(x_0, x_i).\\nonumber\n",
    "\\end{equation}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2 Ex. 6.7"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Derive an expression for the leave-one-out cross-validated residual sum-of-squares for local polynomial regression."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3 Ex. 7.4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the in-sample prediction error (7.18) and the training error \n",
    " \n",
    " in the case of squared-error loss:\n",
    "\n",
    " \\begin{eqnarray}\n",
    "    \\text{Err}_{\\text{in}} &=& \\frac{1}{N}\\sum_{i=1}^NE_{Y^0}(Y_i^0-\\hat f(x_i))^2\\non\\\\\n",
    "    \\overline{\\text{err}} &=& \\frac{1}{N}\\sum_{i=1}^N(y_i-\\hat f(x_i))^2.\\non\n",
    "\\end{eqnarray}\n",
    "\n",
    "Add and substract $f(x_i)$ and $E\\hat f(x_i)$ in each expression and expand. Hence establish that the average optimism in the training error is:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{2}{N}\\sum_{i=1}^N\\text{Cov}(\\hat y_i, y_i),\\non\n",
    "\\end{equation}\n",
    "\n",
    "as given in (7.21)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with $\\text{Err}_{\\text{in}}$. Let's denote $\\hat y_i = \\hat f(x_i)$ and write:\n",
    "\n",
    "\\begin{equation}\n",
    "Y_i^0-\\hat f(x_i) = Y_i^0-f(x_i) + f(x_i)-E\\hat y_i + E\\hat y_i -\\hat y_i\n",
    "\\end{equation}\n",
    "\n",
    "so that\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{Err}{\\text{in}} &=& \\frac{1}{N}\\sum{i=1}^NE_{Y^0}\\left(Y_i^0-f(x_i) + f(x_i)-E\\hat y_i + E\\hat y_i -\\hat y_i\\right)^2\\non\\\n",
    "&=&\\frac{1}{N}\\sum_{i=1}^NA_i + B_i + C_i + D_i + E_i + F_i,\n",
    "\\end{equation}\n",
    "\n",
    "where\n",
    "\n",
    "\\begin{equation}\n",
    "A_i &=& E_{Y^0} (Y_i^0-f(x_i))^2\\non\\\n",
    "B_i &=& E_{Y^0} (f(x_i) - E\\hat y_i)^2 = (f(x_i) - E\\hat y_i)^2\\non\\\n",
    "C_i &=& E_{Y^0} (E\\hat y_i-\\hat y_i)^2 = (E\\hat y_i-\\hat y_i)^2\\non\\\n",
    "D_i &=& 2E_{Y^0} (Y_i^0-f(x_i))(f(x_i) - E\\hat y_i)\\non\\\n",
    "E_i &=& 2E_{Y^0} (Y_i^0-f(x_i))(E\\hat y_i-\\hat y_i)\\non\\\n",
    "F_i &=& 2E_{Y^0} (f(x_i) - E\\hat y_i)(E\\hat y_i-\\hat y_i) = 2(f(x_i) - E\\hat y_i)(E\\hat y_i-\\hat y_i)\\non\n",
    "\\end{equation}\n",
    "\n",
    "Similarly for $\\overline{\\text{err}}$ we have\n",
    "\n",
    "\\begin{equation}\n",
    "y_i-\\hat f(x_i) = y_i - f(x_i) +f(x_i) - E\\hat y_i + E\\hat y_i -\\hat y_i\n",
    "\\end{equation}\n",
    "\n",
    "and\n",
    "\n",
    "\\begin{equation}\n",
    "\\overline{\\text{err}} &=& \\frac{1}{N}\\sum_{i=1}^{N}(y_i - f(x_i) +f(x_i) - E\\hat y_i + E\\hat y_i -\\hat y_i)^2\\non\\\n",
    "&=&\\frac{1}{N}\\sum_{i=1}^N G_i + B_i + C_i + H_i + J_i + F_i,\n",
    "\\end{equation}\n",
    "\n",
    "where\n",
    "\n",
    "\\begin{equation}\n",
    "G_i &=& (y_i-f(x_i))^2\\non\\\n",
    "H_i &=& 2(y_i-f(x_i))(f(x_i) - E\\hat y_i)\\non\\\n",
    "J_i &=& 2(y_i-f(x_i))(E\\hat y_i -\\hat y_i)\\non\n",
    "\\end{equation}\n",
    "\n",
    "Therefore, we have\n",
    "\n",
    "\\begin{equation}\n",
    "E_\\bb{y}(\\text{op}) &=& E_\\bb{y}(\\text{Err}{\\text{in}} - \\overline{\\text{err}})\\non\\\n",
    "&=&\\frac{1}{N}\\sum{i=1}^NE_\\bb{y}[(A_i-G_i) + (D_i-H_i) + (E_i-J_i)].\\non\n",
    "\\end{equation}\n",
    "\n",
    "For $A_i$ and $G_i$ the expectation over $y$ captures unpredictable error, and thus $E_\\bb{y}(A_i-G_i) = 0$. Similarly, we have $E_\\bb{y}(D_i) = E_\\bb{y}(H_i) = E_\\bb{y}(E_i) =0$, and thus\n",
    "\n",
    "\\begin{equation}\n",
    "E_\\bb{y}(\\text{op}) &=& - \\frac{2}{N}\\sum_{i=1}^NJ_i\\non\\\n",
    "&=& - \\frac{2}{N}\\sum_{i=1}^NE_\\bb{y}(y_i-f(x_i))(E\\hat y_i -\\hat y_i)\\non\\\n",
    "&=&\\frac{2}{N}\\sum_{i=1}^N [E_\\bb{y}(y_i\\hat y_i) - E_\\bb{y}(y_i)E_\\bb{y}(\\hat y_i)]\\non\\\n",
    "&=&2\\text{Cov}(y_i, \\hat y_i).\\non\n",
    "\\end{equation}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SOLUS LIBRO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4 Ex. 7.5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a linear smoother, \n",
    "\\begin{equation}\n",
    "\\hat{\\mathbf{y}} = \\mathbf{S} \\mathbf{y}\n",
    "\\end{equation} \n",
    "show that\n",
    "\n",
    "\\begin{equation}\n",
    "\\sum_{i=1}^N\\text{Cov}(\\hat y_i, y_i) = \\text{trace}(\\mathbf{S})\\sigma_\\epsilon^2,\\nonumber\n",
    "\\end{equation}\n",
    "\n",
    "which justifies its use as the effective number of parameters."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\sum_{i=1}^N\\text{Cov}(\\hat y_i, y_i) &= \\text{trace}(\\text{Cov}(\\hat{\\mathbf{y}}, \\mathbf{y}))\\nonumber\\\n",
    "&=\\text{trace}(\\text{Cov}(\\mathbf{\\hat y}, \\mathbf{y}))\\nonumber\\\n",
    "&=\\text{trace}(\\mathbf{\\hat S}\\text{Cov}(\\mathbf{y}, \\mathbf{y}))\\nonumber\\\n",
    "&=\\text{trace}(\\mathbf{\\hat S}\\text{Var}(\\mathbf{y}))\\nonumber\\\n",
    "&=\\text{trace}(\\mathbf{\\hat S})\\sigma_\\epsilon^2.\\nonumber\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SOLUS LIBRO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write a computer program to reproduce Figure 7.3. Use information from Section 7.3.1 to generate the simulated data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
