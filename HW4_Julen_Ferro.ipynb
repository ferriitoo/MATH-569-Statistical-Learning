{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HOMEWORK 4 - JULEN FERRO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1 Ex. 5.4."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the truncated power series representation for cubic splines with $K$ interior knots. Let:\n",
    "\n",
    "\\begin{equation}\n",
    "f(X) = \\sum_{j=0}^3\\beta_jX^j + \\sum_{k=1}^K\\theta_k(X-\\xi_k){+}^3. \\nonumber\n",
    "\\end{equation}\n",
    "\n",
    "Prove that the natural boundary conditions for natural cubic splines (Section 5.2.1) imply the following linear constraints on the coefficients:\n",
    "\n",
    "\\begin{equation}\n",
    "\\beta_2 = 0, \\quad \\sum_{k=1}^K\\theta_k=0 \\nonumber\\  \n",
    "\\beta_3 = 0, \\quad \\sum_{k=1}^K\\xi_k\\theta_k=0. \\nonumber\n",
    "\\end{equation}\n",
    "\n",
    "Hence derive the basis (5.4) and (5.5)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$x<\\xi_{1}$ \n",
    "\n",
    "First of all, it must be taken into account that $f(X)$ is linear when $X < xi_1$ . Then, if the definition of the $f(X)$ function is as following: $f(X) = \\beta_0 + \\beta_1X + \\beta_2X^2 + \\beta_3X^3$ , the condition $X < \\xi_1$ will mean that $\\beta_2=0$ and $\\beta_3=0$ in order to fulfill with the linearity condition, thus vanishing the terms associated with $X^2$ and $X^3$ . \n",
    "\n",
    "\n",
    "But then, similarly:\n",
    "\n",
    "In the case in which $x \\geq \\xi_{K}$:\n",
    "\n",
    "\\begin{equation}\n",
    "f(X) = \\sum_{j=0}^3\\beta_jX^j + \\sum_{k=1}^K\\theta_k(X-\\xi_k)^3 = \\beta_0 + \\beta_1X + 3\\left(\\sum_{k=1}^K\\theta_k\\xi_k^2\\right) X - 3\\left(\\sum_{k=1}^K\\theta_k\\xi_k\\right)X^2 + \\left(\\sum_{k=1}^K\\theta_k\\right)X^3. \\nonumber\n",
    "\\end{equation}\n",
    "\n",
    "$f(X)$ will be linear when $X > xi_K$, then we have:\n",
    "\n",
    "\n",
    "$f(x)=\\sum_{j=0}^{3} \\beta_{j} x^{j}+\\sum_{k=1}^{K} \\theta_{k}\\left(x-\\xi_{k}\\right)^{3}$ and due to the fact that $f$ should be linear there, and because $\\left(x-\\xi_{k}\\right)^{3}=x^{3}-3 \\xi x^{2}+3 \\xi^{2} x-\\xi^{3}$, we have $\\beta_{3}+\\sum_{k=1}^{K} \\theta_{k}=0$ and $\\beta_{2}+\\sum_{k=1}^{K}-3 \\xi_{k} \\theta_{k}=0$, which imply $\\sum_{k=1}^{K} \\theta_{k}=0$ and $\\sum_{k=1}^{K} \\xi_{k} \\theta_{k}=0$.\n",
    "\n",
    "Finally:  \n",
    "\n",
    "\\begin{equation*}\n",
    "\\sum_{k=1}^K\\theta_k = 0 \\quad \\text{and} \\quad \\sum_{k=1}^K\\theta_k\\xi_k = 0.\n",
    "\\end{equation*}\n",
    "\n",
    "As the next step, in order to derive the equations (5.4) and (5.5) as explained in the statement:\n",
    "\n",
    "$(5.4)$  \n",
    "\n",
    "$N_1(X) = 1$, $N_2(X) = X$, $N_{k+2}(X) = d_k(X) - d_{K-1}(X)$  \n",
    "\n",
    "$(5.5)$  \n",
    "\n",
    "$d_k(X) = (X - \\xi_k)^3_+ - (X - \\xi_K)^3_+ + (\\xi_K - \\xi_k)$  \n",
    "\n",
    "\n",
    "Each of these basis functions can be seen to have zero second and third derivative for $X \\geq \\xi_K$.\n",
    "\n",
    "the $f(X)$ function must be written in a specific form, in the shape of aritmetic series following the next structure:\n",
    "\n",
    "\\begin{equation}\n",
    "\\sum_k\\alpha_k N_k(X)\n",
    "\\end{equation}\n",
    "\n",
    "In the specific case of $N_1(X)$ and $N_2(X)$, it can be seen that $alpha_1 = beta_1$ and $alpha_2 = beta_2$. \n",
    "\n",
    "Then, performing the comparation of the coefficient of $(X-\\xi_k)_+^3$ for each of the $k \\le K-2$ , it can be seen that $\\alpha_k = (\\xi_K-\\xi_k)\\theta_k$.\n",
    "\n",
    "As the next step, we will train to get an expression for: \n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\sum_{k=1}^{K-2}\\alpha_kN_{k+2}(X)\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\sum_{k=1}^{K-2}\\alpha_kN_{k+2}(X) &= \\sum_{k=1}^{K-2}(\\xi_K-\\xi_k)\\theta_k\\left[\\frac{(X-\\xi_k)+^3-(X-\\xi_K)+^3}{\\xi_K-\\xi_k} - \\frac{(X-\\xi_{K-1})+^3-(X-\\xi_K)+^3}{\\xi_K-\\xi_{K-1}}\\right] \\\n",
    "&=\\sum_{k=1}^{K-2}\\theta_k(X-\\xi_k)+^3 -\\left(\\sum{k=1}^{K-2}\\theta_k\\right)(X-\\xi_K)+^3 \\\n",
    "&\\quad - \\frac{1}{\\xi_K-\\xi{K-1}}\\left[\\xi_K\\left(\\sum_{k=1}^{K-2}\\theta_k\\right) - \\sum_{k=1}^{K-2}\\xi_k\\theta_k\\right][(X-\\xi_{K-1})+^3-(X-\\xi_K)+^3].\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "Then:  \n",
    "\n",
    "\\begin{align}\n",
    "\\sum_{k=1}^{K-2}\\theta_k&=-\\theta_{K-1}-\\theta_K \n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "\\sum_{k=1}^{K-2}\\xi_k\\theta_k&=-\\theta_{K-1}\\xi_{K-1} - \\theta_K\\xi_K\n",
    "\\end{align}\n",
    "\n",
    "Applying these to terms in the last equation, we get our final results:  \n",
    "\n",
    "\\begin{align}\n",
    "&\\sum_{k=1}^{K-2}\\theta_k(x-\\xi_k)+^3 + (\\theta{K-1}+\\theta_K)(x-\\xi_K)+^3 \\nonumber\\\n",
    "&\\qquad- \\frac{(-\\xi_K\\theta{K-1}-\\xi_K\\theta_K + \\xi_{K-1}\\theta_{K-1} + \\xi_K\\theta_K)}{\\xi_K-\\xi_{K-1}}[(x-\\xi_{K-1})+^3-(x-\\xi_K)+^3]\\nonumber\\\n",
    "&=\\sum_{k=1}^{K-2}\\theta_k(x-\\xi_k)+^3 + (\\theta{K-1}+\\theta_K)(x-\\xi_K)+^3\\nonumber\\\n",
    "&\\qquad+\\theta{K-1}[(x-\\xi_{K-1})+^3-(x-\\xi_K)+^3]\\nonumber\\\n",
    "&=\\sum_{k=1}^K\\theta_k(x-\\xi_k)_+^3.\\nonumber\n",
    "\\end{align}\n",
    "\n",
    "At this point, we have already verified that:\n",
    "\\begin{equation}\n",
    "\\sum_{k=1}^{K-2}\\alpha_kN_{k+2}(x) = \\sum_{k=1}^K\\theta_k(x-\\xi_k)_{+}^3. \\nonumber\n",
    "\\end{equation}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the point of the solution to this exercise is to show that the next two ways of defining natural cubic splines are equivalent:\n",
    "\n",
    "1. By the basis in (5.4), with the definition in (5.5).\n",
    "\n",
    "2. By the truncated power series representation in (5.70), with the natural boundary conditions in (5.71).\n",
    "\n",
    "In the direction $(1) \\Rightarrow(2)$. It is immediate that a function described using the basis in (5.4) can also be written in the form (5.70). What remains is to show that the natural boundary conditions of (5.71) are satisfied. For $x<\\xi_{1}$, all $N_{k+2}(x)$ are 0 , so the function is linear. For $x \\geq \\xi_{K}$ and $k \\leq K-2, N_{k+2}(x)$ evaluates to:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\frac{\\left(x^{3}-3 \\xi_{k} x^{2}+3 \\xi_{k}^{2} x-\\xi_{k}^{3}\\right)-\\left(x^{3}-3 \\xi_{K} x^{2}+3 \\xi_{K}^{2} x-\\xi_{K}^{3}\\right)}{\\xi_{K}-\\xi_{k}}- \\\\\n",
    "& \\frac{\\left(x^{3}-3 \\xi_{K-1} x^{2}+3 \\xi_{K-1}^{2} x-\\xi_{K-1}^{3}\\right)-\\left(x^{3}-3 \\xi_{K} x^{2}+3 \\xi_{K}^{2} x-\\xi_{K}^{3}\\right)}{\\xi_{K}-\\xi_{K-1}}= \\\\\n",
    "& \\frac{\\left(-3 \\xi_{k}+3 \\xi_{K}\\right)}{\\xi_{K}-\\xi_{k}} x^{2}-\\frac{\\left(-3 \\xi_{K-1}+3 \\xi_{K}\\right)}{\\xi_{K}-\\xi_{K-1}} x^{2}+C_{1} x+C_{0}=C_{1} x+C_{0}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "for constants $C_{1}$ and $C_{0}$, so that once more the function is linear.\n",
    "\n",
    "In the direction (2) $\\Rightarrow$ (1). Assume a function is written as in (5.70) and (5.71). Then for the basis in (5.4), the corresponding coefficient of $N_{k+2}(x)$ is $\\left(\\xi_{K}-\\xi_{k}\\right) \\theta_{k}$ for each $k \\leq K-2$, and obviously the coefficients of $N_{1}(x)$ and $N_{2}(x)$ are $\\beta_{0}$ and $\\beta_{1}$. Let us verify that we thereby obtain the same function, looking only at the coefficients of the $N_{k+2}(x)$ :\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\sum_{k=1}^{K-2}\\left(\\xi_{K}-\\xi_{k}\\right) \\theta_{k}\\left[\\frac{\\left(x-\\xi_{k}\\right)_{+}^{3}-\\left(x-\\xi_{K}\\right)_{+}^{3}}{\\xi_{K}-\\xi_{k}}-\\frac{\\left(x-\\xi_{K-1}\\right)_{+}^{3}-\\left(x-\\xi_{K}\\right)_{+}^{3}}{\\xi_{K}-\\xi_{K-1}}\\right]= \\\\\n",
    "& \\sum_{k=1}^{K-2} \\theta_{k}\\left(x-\\xi_{k}\\right)_{+}^{3}-\\sum_{k=1}^{K-2} \\theta_{k}\\left(x-\\xi_{K}\\right)_{+}^{3}- \\\\\n",
    "& \\frac{1}{\\xi_{K}-\\xi_{K-1}}\\left[\\sum_{k=1}^{K-2} \\xi_{K} \\theta_{k}-\\sum_{k=1}^{K-2} \\xi_{k} \\theta_{k}\\right]\\left[\\left(x-\\xi_{K-1}\\right)_{+}^{3}-\\left(x-\\xi_{K}\\right)_{+}^{3}\\right]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Now we make use of (5.71), in particular $\\sum_{k=1}^{K-2} \\theta_{k}=-\\theta_{K-1}-\\theta_{K}$ and $\\sum_{k=1}^{K-2} \\xi_{k} \\theta_{k}=$ $-\\xi_{K-1} \\theta_{K-1}-\\xi_{K} \\theta_{K}$ and obtain:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\sum_{k=1}^{K-2} \\theta_{k}\\left(x-\\xi_{k}\\right)_{+}^{3}+\\left(\\theta_{K-1}+\\theta_{K}\\right)\\left(x-\\xi_{K}\\right)_{+}^{3}- \\\\\n",
    "& \\quad \\frac{1}{\\xi_{K}-\\xi_{K-1}}\\left[-\\xi_{K} \\theta_{K-1}-\\xi_{K} \\theta_{K}+\\xi_{K-1} \\theta_{K-1}+\\xi_{K} \\theta_{K}\\right]\\left[\\left(x-\\xi_{K-1}\\right)_{+}^{3}-\\left(x-\\xi_{K}\\right)_{+}^{3}\\right]= \\\\\n",
    "& \\sum_{k=1}^{K-2} \\theta_{k}\\left(x-\\xi_{k}\\right)_{+}^{3}+\\left(\\theta_{K-1}+\\theta_{K}\\right)\\left(x-\\xi_{K}\\right)_{+}^{3}- \\\\\n",
    "& \\quad \\frac{-\\xi_{K} \\theta_{K-1}+\\xi_{K-1} \\theta_{K-1}}{\\xi_{K}-\\xi_{K-1}}\\left[\\left(x-\\xi_{K-1}\\right)_{+}^{3}-\\left(x-\\xi_{K}\\right)_{+}^{3}\\right]= \\\\\n",
    "& \\sum_{k=1}^{K-2} \\theta_{k}\\left(x-\\xi_{k}\\right)_{+}^{3}+\\theta_{K-1}\\left(x-\\xi_{K}\\right)_{+}^{3}+\\theta_{K}\\left(x-\\xi_{K}\\right)_{+}^{3}+\\theta_{K-1}\\left(x-\\xi_{K-1}\\right)_{+}^{3}-\\theta_{K-1}\\left(x-\\xi_{K}\\right)_{+}^{3}= \\\\\n",
    "& \\sum_{k=1}^{K} \\theta_{k}\\left(x-\\xi_{k}\\right)_{+}^{3}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "as required."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2 Ex. 5.15"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This exercise derives some of the results quoted in Section 5.8.1. Suppose K(x, y) satisfying the conditions (5.45) and let f(x) ∈ HK. Show that:\n",
    "\n",
    "a) $\\langle K(\\cdot, x_i), f\\rangle_{\\mathcal{H}_K} = f(x_i)$\n",
    "\n",
    "b) $\\langle K(\\cdot, x_i), K(\\cdot, x_j)\\rangle_{\\mathcal{H}_K} = K(x_i, x_j)$\n",
    "\n",
    "c) $g(x) = \\sum_{i=1}^N\\alpha_iK(x,x_i)$\n",
    "\n",
    "\\begin{equation}\n",
    "J(g) = \\sum_{i=1}^N\\sum_{j=1}^NK(x_i, x_j)\\alpha_i\\alpha_j.\n",
    "\\end{equation}\n",
    "\n",
    "Suppose that $tilde g(x) = g(x) + \\rho(x)$, with $rho(x) \\in \\mathcal{H}_K$, and orthogonal in HK to each of $K(x, x_i)$, $i = 1,...,N$. Show that: \n",
    "\n",
    "d) \n",
    "\n",
    "\\begin{equation}\n",
    "\\sum_{i=1}^NL(y_i, \\tilde g(x_i)) + \\lambda J(\\tilde g) \\ge \\sum_{i=1}^NL(y_i, g(x_i)) + \\lambda J(g)\n",
    "\\end{equation}\n",
    "\n",
    "with equality iff $\\rho(x) = 0$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This exercise derives some of the results quoted in Section 5.8.1. Suppose K(x, y) satisfying the conditions (5.45) and let f(x) ∈ HK. Show that:\n",
    "\n",
    "a) $\\langle K(\\cdot, x_i), f\\rangle_{\\mathcal{H}_K} = f(x_i)$\n",
    "\n",
    "In order to show the next derivations, we will be using the inner product $H_K$:\n",
    "\n",
    "\\begin{align}\n",
    "\\left\\langle \\sum_{j\\in J}a_j\\phi_j(x), \\sum_{j\\in J}b_j\\phi_j(x) \\right\\rangle_{\\mathcal{H}K} &= \\sum{j\\in J}a_jb_j\\langle\\phi_j,\\phi_j\\rangle_{\\mathcal{H}K} \\nonumber\\\n",
    "&= \\sum{j\\in J}a_jb_j\\frac{1}{\\lambda_j} \\nonumber\\\n",
    "&= \\sum_{j\\in J}\\frac{a_jb_j}{\\lambda_j}. \\nonumber\n",
    "\\end{align}\n",
    "\n",
    "a) Using the above equation:\n",
    "\n",
    "\\begin{align}\n",
    "\\langle K(\\cdot, y), f\\rangle_{\\mathcal{H}K} &= \\left\\langle \\sum{i=1}^\\infty (\\gamma_i\\phi_i(x))\\phi_i(y), \\sum_{i=1}^\\infty c_i\\phi_i(x)\\right\\rangle \\nonumber\\\n",
    "&= \\sum_{i=1}^\\infty c_i\\phi_i(y) \\nonumber\\\n",
    "&= f(y). \\nonumber\n",
    "\\end{align}\n",
    "\n",
    "where we set $f(\\cdot) = \\sum_{j=1}^\\infty c_j \\phi_j(\\cdot) \\in \\mathcal{H}_K$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) $\\langle K(\\cdot, x_i), K(\\cdot, x_j)\\rangle_{\\mathcal{H}_K} = K(x_i, x_j)$\n",
    "\n",
    "b) same as in 'a' but setting the $f$ term as follows:\n",
    "\n",
    "\n",
    "$f(\\cdot) = K(\\cdot, x_j)$\n",
    "\n",
    "\\begin{align}\n",
    "\\langle K(\\cdot, x_i), K(\\cdot, x_j)\\rangle_{\\mathcal{H}K} &= \\sum{k=1}^\\infty \\langle \\gamma_k \\phi_k(\\cdot), K(\\cdot, x_i)\\rangle_{\\mathcal{H}K} \\gamma_k \\langle \\phi_k(\\cdot), K(\\cdot, x_j)\\rangle{\\mathcal{H}K} \\nonumber \\\n",
    "&= \\sum{k=1}^\\infty \\gamma_k^2 K(x_i, x_k) K(x_j, x_k) \\nonumber \\\n",
    "&= K(x_i, x_j).\n",
    "\\end{align}\n",
    "\n",
    "SORRY ABOUT THE ' 1 ^^ INFINITE ' TERMS, BUT I CANNOT MAKE THE EQUATIONS WORK IN A PROPER WAY.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) $g(x) = \\sum_{i=1}^N\\alpha_iK(x,x_i)$\n",
    "\n",
    "\\begin{equation}\n",
    "J(g) = \\sum_{i=1}^N\\sum_{j=1}^NK(x_i, x_j)\\alpha_i\\alpha_j.\n",
    "\\end{equation}\n",
    "\n",
    "Suppose that $tilde g(x) = g(x) + \\rho(x)$, with $rho(x) \\in \\mathcal{H}_K$, and orthogonal in HK to each of $K(x, x_i)$, $i = 1,...,N$. Show that: \n",
    "\n",
    "\n",
    "\n",
    "c) \n",
    "\n",
    "\\begin{align}\n",
    "J(g) &= \\left \\langle \\sum_{i=1}^N\\alpha_iK(x, x_i), \\sum_{j=1}^N\\alpha_jK(x, x_j) \\right\\rangle \\nonumber \\\n",
    "&= \\sum_{i=1}^N\\sum_{j=1}^NK(x_i, x_j)\\alpha_i\\alpha_j. \\nonumber\n",
    "\\end{align}\n",
    "\n",
    "Using the definition of $g(x)$ and the inner product formula, we have:\n",
    "\n",
    "\\begin{align}\n",
    "J(g) &= \\langle g, g \\rangle_{\\mathcal{H}K} = \\left\\langle \\sum{i=1}^N \\alpha_i K(\\cdot, x_i), \\sum_{j=1}^N \\alpha_j K(\\cdot, x_j)\\right\\rangle_{\\mathcal{H}K} \\nonumber \\\n",
    "&= \\sum{i=1}^N \\sum_{j=1}^N \\alpha_i \\alpha_j K(x_i, x_j).\n",
    "\\end{align}\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) \n",
    "\n",
    "\\begin{equation}\n",
    "\\sum_{i=1}^NL(y_i, \\tilde g(x_i)) + \\lambda J(\\tilde g) \\ge \\sum_{i=1}^NL(y_i, g(x_i)) + \\lambda J(g)\n",
    "\\end{equation}\n",
    "\n",
    "with equality iff $\\rho(x) = 0$\n",
    "\n",
    "\n",
    "d) \n",
    "\n",
    "The results from part (a) state that:\n",
    "\n",
    "\\begin{align}\n",
    "\\tilde g(x_i) &= \\langle K(\\cdot, x_i), \\tilde g \\rangle_{\\mathcal{H}K} \\nonumber \\\n",
    "&= \\langle K(\\cdot, x_i), g + \\rho \\rangle{\\mathcal{H}K} \\nonumber \\\n",
    "&= \\langle K(\\cdot, x_i), g \\rangle{\\mathcal{H}_K}, \\nonumber\n",
    "\\end{align}\n",
    "\n",
    "Therefore,\n",
    "\n",
    "\\begin{equation}\n",
    "L(y_i, \\tilde g(x_i)) = L(y_i, g(x_i)). \\nonumber\n",
    "\\end{equation}\n",
    "\n",
    "Since both losses are equal, we can simplify the equation we want to derive by removing some terms:\n",
    "\n",
    "\\begin{align}\n",
    "\\tilde g(x_i) &= \\langle K(\\cdot, x_i), \\tilde g \\rangle_{\\mathcal{H}K} \\nonumber \\\n",
    "&= \\langle K(\\cdot, x_i), g + \\rho \\rangle{\\mathcal{H}K} \\nonumber \\\n",
    "&= \\langle K(\\cdot, x_i), g \\rangle{\\mathcal{H}_K}. \\nonumber\n",
    "\\end{align}\n",
    "\n",
    "Next, taking into account that $\\rho$ is orthogonal to all of the $K(x, x_i)$ for $i = 1,...,N$...\n",
    "\n",
    "We know that $\\tilde g(x) = g(x) + \\rho(x)$, where $\\rho(x)$ is orthogonal to each of the functions $K(\\cdot, x_i)$. Then, we have:\n",
    "\n",
    "\\begin{align}\n",
    "\\tilde g(x_i) &= \\langle K(\\cdot, x_i), \\tilde g \\rangle_{\\mathcal{H}K} \\nonumber \\\n",
    "&= \\langle K(\\cdot, x_i), g + \\rho \\rangle{\\mathcal{H}K} \\nonumber \\\n",
    "&= \\langle K(\\cdot, x_i), g \\rangle{\\mathcal{H}_K},\n",
    "\\end{align}\n",
    "\n",
    "since $\\rho$ is orthogonal to $K(\\cdot, x_i)$.\n",
    "\n",
    "We have:\n",
    "\n",
    "\\begin{align}\n",
    "\\lambda J(\\tilde g) &= \\lambda J(g + \\rho) \\nonumber \\\n",
    "&= \\lambda \\langle g + \\rho, g + \\rho \\rangle_{\\mathcal{H}K} \\nonumber \\\n",
    "&= \\lambda \\langle g, g \\rangle{\\mathcal{H}K} + \\lambda \\langle \\rho, \\rho \\rangle{\\mathcal{H}K} + 2\\lambda \\langle g, \\rho \\rangle{\\mathcal{H}K} \\nonumber \\\n",
    "&= \\lambda J(g) + \\lambda |\\rho|^2{\\mathcal{H}_K}. \\nonumber\n",
    "\\end{align}\n",
    "\n",
    "Therefore,\n",
    "\n",
    "\\begin{equation}\n",
    "\\lambda J(\\tilde g) \\ge \\lambda J(g). \\nonumber\n",
    "\\end{equation}\n",
    "\n",
    "This completes the proof of statement (d)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3 "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a program to reproduce Figure 5.3 on page 145. The South Africa Heart Disease data are provided. To understand how the variance is calculated, read section 5.2.2."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import ShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data from URL\n",
    "path = 'https://raw.githubusercontent.com/ferriitoo/csv/main/SAheart.data.txt'\n",
    "df = pd.read_csv(path, sep=',', index_col=0)\n",
    "df.head(10)\n",
    "\n",
    "# Define the predictor variables\n",
    "X = df[['tobacco', 'ldl', 'adiposity', 'famhist', 'typea', 'obesity', 'alcohol', 'age']]\n",
    "\n",
    "# Define the response variable\n",
    "y = df['chd']\n",
    "\n",
    "# Create dummy variables for the 'famhist' column\n",
    "famhist_dummies = pd.get_dummies(X['famhist'], prefix='famhist')\n",
    "\n",
    "# Drop the original 'famhist' column and concatenate the dummy variables\n",
    "X = pd.concat([X.drop('famhist', axis=1), famhist_dummies], axis=1)\n",
    "\n",
    "# Create a logistic regression model object\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Define the cross-validation splitter\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\n",
    "\n",
    "# Create learning curve\n",
    "train_sizes, train_scores, test_scores = learning_curve(model, X, y, cv=cv, n_jobs=-1, train_sizes=np.linspace(0.1, 1.0, 10))\n",
    "\n",
    "# Calculate mean and standard deviation of training and testing scores\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "mode": "lines+markers",
         "name": "Global Linear",
         "type": "scatter",
         "x": [
          0.020584494295802447,
          0.034388521115218396,
          0.046450412719997725,
          0.05808361216819946,
          0.06505159298527952,
          0.09767211400638387,
          0.12203823484477883,
          0.13949386065204183,
          0.15599452033620265,
          0.15601864044243652,
          0.17052412368729153,
          0.18182496720710062,
          0.18340450985343382,
          0.18485445552552704,
          0.19967378215835974,
          0.21233911067827616,
          0.2587799816000169,
          0.2912291401980419,
          0.29214464853521815,
          0.3042422429595377,
          0.3046137691733707,
          0.31171107608941095,
          0.3663618432936917,
          0.3745401188473625,
          0.43194501864211576,
          0.4401524937396013,
          0.45606998421703593,
          0.4951769101112702,
          0.5142344384136116,
          0.5200680211778108,
          0.5247564316322378,
          0.5467102793432796,
          0.5924145688620425,
          0.5986584841970366,
          0.6011150117432088,
          0.6075448519014384,
          0.6118528947223795,
          0.662522284353982,
          0.6842330265121569,
          0.7080725777960455,
          0.7319939418114051,
          0.7851759613930136,
          0.8083973481164611,
          0.8324426408004217,
          0.8661761457749352,
          0.9093204020787821,
          0.9488855372533332,
          0.9507143064099162,
          0.9656320330745594,
          0.9699098521619943
         ],
         "y": [
          0.06424157642855223,
          0.061416529762565876,
          0.059024313861873165,
          0.056784528646415776,
          0.05547465232117391,
          0.04965828678456305,
          0.04565327462726243,
          0.04296265130702753,
          0.0405562461958196,
          0.04055282606495367,
          0.03854754890418395,
          0.0370566073369938,
          0.03685319060465478,
          0.03666753782384832,
          0.034829013281146966,
          0.03334284615410797,
          0.028564673310033074,
          0.025852084309221257,
          0.025783022135829115,
          0.024908927596685393,
          0.024883216378557935,
          0.02440501510105407,
          0.021548001563962374,
          0.021246116065414725,
          0.020047786415555754,
          0.020008145604096444,
          0.020025174212527502,
          0.020593232345486046,
          0.02114112890322581,
          0.021344351193264374,
          0.021519743322158225,
          0.02248406522176155,
          0.025247823772209688,
          0.02570471685224579,
          0.02588969775830674,
          0.026387849002985825,
          0.026732926689243514,
          0.03147280871460861,
          0.03388803103328766,
          0.0368056251859577,
          0.040012626504713794,
          0.048145199013363005,
          0.052130074492092504,
          0.05653427107753983,
          0.06318962382638993,
          0.07251275927924661,
          0.08186272682622273,
          0.0823134105944584,
          0.0860508428543369,
          0.08714267273881468
         ]
        },
        {
         "mode": "lines+markers",
         "name": "Global Cubic Spline",
         "type": "scatter",
         "x": [
          0.020584494295802447,
          0.034388521115218396,
          0.046450412719997725,
          0.05808361216819946,
          0.06505159298527952,
          0.09767211400638387,
          0.12203823484477883,
          0.13949386065204183,
          0.15599452033620265,
          0.15601864044243652,
          0.17052412368729153,
          0.18182496720710062,
          0.18340450985343382,
          0.18485445552552704,
          0.19967378215835974,
          0.21233911067827616,
          0.2587799816000169,
          0.2912291401980419,
          0.29214464853521815,
          0.3042422429595377,
          0.3046137691733707,
          0.31171107608941095,
          0.3663618432936917,
          0.3745401188473625,
          0.43194501864211576,
          0.4401524937396013,
          0.45606998421703593,
          0.4951769101112702,
          0.5142344384136116,
          0.5200680211778108,
          0.5247564316322378,
          0.5467102793432796,
          0.5924145688620425,
          0.5986584841970366,
          0.6011150117432088,
          0.6075448519014384,
          0.6118528947223795,
          0.662522284353982,
          0.6842330265121569,
          0.7080725777960455,
          0.7319939418114051,
          0.7851759613930136,
          0.8083973481164611,
          0.8324426408004217,
          0.8661761457749352,
          0.9093204020787821,
          0.9488855372533332,
          0.9507143064099162,
          0.9656320330745594,
          0.9699098521619943
         ],
         "y": [
          0.224974536259658,
          0.17896506441955032,
          0.14657780086578945,
          0.12129752536918673,
          0.10862176394255364,
          0.06897487132676251,
          0.05498247981941567,
          0.05036833172176576,
          0.04883088866964536,
          0.04883018526767609,
          0.04903015202391859,
          0.04985408133828225,
          0.0500028156596701,
          0.05014527041021024,
          0.05184543745373236,
          0.053483695505206263,
          0.058551246426068036,
          0.05975328500874425,
          0.05975361378840424,
          0.05958995433352631,
          0.0595801010650689,
          0.059339047621850056,
          0.05486148556992915,
          0.05394740318205054,
          0.0479292972592595,
          0.047305663182247326,
          0.04638483044899721,
          0.04607592508161983,
          0.04704195287521083,
          0.0474856690474099,
          0.04789136614509747,
          0.050343360597648504,
          0.057786671758714284,
          0.058964549905068456,
          0.05943360981476915,
          0.060672380610167685,
          0.06150835710134545,
          0.07085284098771369,
          0.0740009584088024,
          0.07644135982491004,
          0.07764205115948954,
          0.07630486820988551,
          0.0750745597051495,
          0.07485105183137136,
          0.07986390984072562,
          0.10662667065461294,
          0.17016136547560132,
          0.1744186852790392,
          0.21464014281708743,
          0.2281368649295517
         ]
        },
        {
         "mode": "lines+markers",
         "name": "Cubic Spline - 2 knots",
         "type": "scatter",
         "x": [
          0.020584494295802447,
          0.034388521115218396,
          0.046450412719997725,
          0.05808361216819946,
          0.06505159298527952,
          0.09767211400638387,
          0.12203823484477883,
          0.13949386065204183,
          0.15599452033620265,
          0.15601864044243652,
          0.17052412368729153,
          0.18182496720710062,
          0.18340450985343382,
          0.18485445552552704,
          0.19967378215835974,
          0.21233911067827616,
          0.2587799816000169,
          0.2912291401980419,
          0.29214464853521815,
          0.3042422429595377,
          0.3046137691733707,
          0.31171107608941095,
          0.3663618432936917,
          0.3745401188473625,
          0.43194501864211576,
          0.4401524937396013,
          0.45606998421703593,
          0.4951769101112702,
          0.5142344384136116,
          0.5200680211778108,
          0.5247564316322378,
          0.5467102793432796,
          0.5924145688620425,
          0.5986584841970366,
          0.6011150117432088,
          0.6075448519014384,
          0.6118528947223795,
          0.662522284353982,
          0.6842330265121569,
          0.7080725777960455,
          0.7319939418114051,
          0.7851759613930136,
          0.8083973481164611,
          0.8324426408004217,
          0.8661761457749352,
          0.9093204020787821,
          0.9488855372533332,
          0.9507143064099162,
          0.9656320330745594,
          0.9699098521619943
         ],
         "y": [
          0.3671377672875418,
          0.23893327010744736,
          0.16855576159876376,
          0.1270799396269549,
          0.11137967446336347,
          0.08905337483341694,
          0.09263795343574599,
          0.09417171555708075,
          0.09246342451946471,
          0.0924585501096396,
          0.0884389638069438,
          0.0841506536371271,
          0.08350129278783974,
          0.08289829698248029,
          0.07663193274003426,
          0.07176547629985104,
          0.06776968236751406,
          0.08111005887853123,
          0.08162239147832842,
          0.08867883249152055,
          0.08889955269223128,
          0.09308362377402499,
          0.10958857724325889,
          0.10875344564727635,
          0.08771715901031282,
          0.08399219768687642,
          0.0773772208955836,
          0.06794563378876466,
          0.0679517973703354,
          0.06856360554546086,
          0.06924793508102739,
          0.07440812947676081,
          0.08982247824293473,
          0.09170537648379741,
          0.0923969288767411,
          0.09405341745114272,
          0.09502496315958567,
          0.09656995493571262,
          0.09415562980628839,
          0.09586788301740541,
          0.10649343979318432,
          0.16079071119587704,
          0.18656489893981795,
          0.20323356635363238,
          0.19671875696937782,
          0.14846399034622945,
          0.1795872090300822,
          0.18694310115838242,
          0.28142161923966347,
          0.32221818377864053
         ]
        },
        {
         "mode": "lines+markers",
         "name": "Natural Cubic Spline - 6 knots",
         "type": "scatter",
         "x": [
          0.020584494295802447,
          0.034388521115218396,
          0.046450412719997725,
          0.05808361216819946,
          0.06505159298527952,
          0.09767211400638387,
          0.12203823484477883,
          0.13949386065204183,
          0.15599452033620265,
          0.15601864044243652,
          0.17052412368729153,
          0.18182496720710062,
          0.18340450985343382,
          0.18485445552552704,
          0.19967378215835974,
          0.21233911067827616,
          0.2587799816000169,
          0.2912291401980419,
          0.29214464853521815,
          0.3042422429595377,
          0.3046137691733707,
          0.31171107608941095,
          0.3663618432936917,
          0.3745401188473625,
          0.43194501864211576,
          0.4401524937396013,
          0.45606998421703593,
          0.4951769101112702,
          0.5142344384136116,
          0.5200680211778108,
          0.5247564316322378,
          0.5467102793432796,
          0.5924145688620425,
          0.5986584841970366,
          0.6011150117432088,
          0.6075448519014384,
          0.6118528947223795,
          0.662522284353982,
          0.6842330265121569,
          0.7080725777960455,
          0.7319939418114051,
          0.7851759613930136,
          0.8083973481164611,
          0.8324426408004217,
          0.8661761457749352,
          0.9093204020787821,
          0.9488855372533332,
          0.9507143064099162,
          0.9656320330745594,
          0.9699098521619943
         ],
         "y": [
          0.24785310619125714,
          0.20276135548221183,
          0.16769506467659792,
          0.1306839907850172,
          0.12320746922403018,
          0.07912979934873438,
          0.07596260964139646,
          0.07913152771167888,
          0.0893579084994758,
          0.0903072191441197,
          0.10204401488271944,
          0.09202944537672143,
          0.09235932149047033,
          0.09306798210793613,
          0.0923599462306458,
          0.0711064709917495,
          0.06111466619307119,
          0.09936577115921037,
          0.09117126188533982,
          0.11442030468897346,
          0.09893764267654107,
          0.14121603424379345,
          0.14338882878107115,
          0.14127432891871902,
          0.09370506759140615,
          0.09130626760077637,
          0.10220097979726203,
          0.11839950462205884,
          0.11262940597942331,
          0.10856001675480405,
          0.11994128605791081,
          0.09481342397565432,
          0.05446087119414865,
          0.06544328695210626,
          0.05332124646425469,
          0.05985647113005569,
          0.07137028188038391,
          0.09956243867969848,
          0.12373600657872746,
          0.1098665059784491,
          0.1088475145613958,
          0.09701823814737144,
          0.08388585778997289,
          0.07464506553551015,
          0.081740357319283,
          0.11917514898567966,
          0.18545237664570394,
          0.19186979076611746,
          0.22177648531853209,
          0.23638651896458326
         ]
        }
       ],
       "layout": {
        "legend": {
         "x": 0.5,
         "xanchor": "center",
         "y": 0.99,
         "yanchor": "top"
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "xaxis": {
         "title": {
          "text": "x"
         }
        },
        "yaxis": {
         "title": {
          "text": "Pointwise Variance"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from patsy import dmatrix\n",
    "import statsmodels.api as sm\n",
    "import plotly.graph_objects as go\n",
    "from numpy.linalg import inv\n",
    "\n",
    "# generate data\n",
    "np.random.seed(42)\n",
    "x = np.random.uniform(0, 1, 50)\n",
    "x = np.sort(x)\n",
    "err = np.random.normal(0, 1, 50)\n",
    "y = np.add(x, err)\n",
    "\n",
    "\n",
    "# linear regression\n",
    "def GLCov(x, sigma):\n",
    "    x_m = np.array([np.ones(len(x)), x]).transpose()\n",
    "    x_m_t = x_m.transpose()\n",
    "    x_c = np.matmul(x_m_t, x_m)\n",
    "    x_c_inv = inv(x_c)\n",
    "    x_c_inv = x_c_inv * (sigma * sigma)\n",
    "    return x_c_inv\n",
    "\n",
    "\n",
    "cov = GLCov(x, 1)\n",
    "pt_var = cov[0][0] + cov[1][1] * x * x + 2 * cov[0][1] * x\n",
    "\n",
    "# global cubic\n",
    "from numpy.linalg import inv\n",
    "from numpy.linalg import multi_dot\n",
    "\n",
    "\n",
    "def GlobalCubicCov(x, sigma):\n",
    "    x_m = np.array([np.ones(len(x)), x, x * x, x * x * x]).transpose()\n",
    "    x_m_t = x_m.transpose()\n",
    "    x_c = np.matmul(x_m_t, x_m)\n",
    "    x_c_inv = inv(x_c)\n",
    "    x_c_inv = x_c_inv * (sigma * sigma)\n",
    "    return x_c_inv\n",
    "\n",
    "\n",
    "x_square = x * x\n",
    "x_cubic = x * x * x\n",
    "m = GlobalCubicCov(x, 1)\n",
    "x_m = np.array([np.ones(len(x)), x, x * x, x * x * x]).transpose()\n",
    "res = multi_dot([x_m, m, x_m.transpose()])\n",
    "pt_var_cubic = res.diagonal()\n",
    "\n",
    "# Fit a cubic spline with two knots at 0.33 and 0.66\n",
    "x_cubic = dmatrix('bs(x, knots=(0.33, 0.66))', {'x': x})\n",
    "fit_cubic = sm.GLM(y, x_cubic).fit()\n",
    "\n",
    "# Fit a natural spline with lower and upper bounds\n",
    "x_natural = dmatrix('cr(x, df=6, lower_bound=0.1, upper_bound=0.9)', {'x': x})\n",
    "fit_natural = sm.GLM(y, x_natural).fit()\n",
    "\n",
    "# Create spline lines for 50 evenly spaced values of age\n",
    "# line_cubic = fit_cubic.predict(dmatrix('bs(xp, knots=(0.33, 0.66))', {'xp': x}))\n",
    "# line_natural = fit_natural.predict(dmatrix('cr(xp, df=6)', {'xp': x}))\n",
    "\n",
    "# natural cubic spline\n",
    "H = np.asarray(x_natural)\n",
    "sigma = 1\n",
    "m_Sigma = sigma * sigma * (inv(np.matmul(H.transpose(), H)))\n",
    "m_cubic_natural = multi_dot([H, m_Sigma, H.transpose()])\n",
    "res_cubic_natural = m_cubic_natural.diagonal()\n",
    "\n",
    "# cubic spline\n",
    "H = np.asarray(x_cubic)\n",
    "sigma = 1\n",
    "m_Sigma = sigma * sigma * (inv(np.matmul(H.transpose(), H)))\n",
    "m_cubic = multi_dot([H, m_Sigma, H.transpose()])\n",
    "res_cubic = m_cubic.diagonal()\n",
    "\n",
    "# Create traces\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=x, y=pt_var,\n",
    "                        mode='lines+markers',\n",
    "                        name='Global Linear'))\n",
    "fig.add_trace(go.Scatter(x=x, y=pt_var_cubic,\n",
    "                        mode='lines+markers',\n",
    "                        name='Global Cubic Spline'))\n",
    "fig.add_trace(go.Scatter(x=x, y=res_cubic,\n",
    "                        mode='lines+markers', name='Cubic Spline - 2 knots'))\n",
    "fig.add_trace(go.Scatter(x=x, y=res_cubic_natural,\n",
    "                        mode='lines+markers', name='Natural Cubic Spline - 6 knots'))\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis_title=\"x\",\n",
    "    yaxis_title=\"Pointwise Variance\",\n",
    ")\n",
    "\n",
    "fig.update_layout(legend=dict(\n",
    "    yanchor=\"top\",\n",
    "    y=0.99,\n",
    "    xanchor=\"center\",\n",
    "    x=0.5\n",
    "))\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4 Ex. 6.1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Show that the Nadaraya–Watson kernel smooth with fixed metric bandwidth λ and a Gaussian kernel is differentiable.  \n",
    "b) What can be said for the Epanechnikov kernel? What can be said for the Epanechnikov kernel with adaptive nearest-neighbor bandwidth λ(x0)?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) The Nadaraya-Watson kernel-weighted average expression:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat f(x_0) = \\frac{\\sum_{i=1}^NK_\\lambda(x_0, x_i)y_i}{\\sum_{i=1}^NK_\\lambda(x_0, x_i)}.\n",
    "\\end{equation}\n",
    "\n",
    "The Gaussian kernel:\n",
    "\n",
    "\\begin{equation}\n",
    "K_\\lambda(x_0, x) = \\frac{1}{\\sqrt{2\\pi}\\lambda}\\exp\\left(-\\frac{(x-x_0)^2}{2\\lambda^2}\\right),\n",
    "\\end{equation}\n",
    "\n",
    "is known to be differentiable in $x_0$ for all different values of $x_0$ since $K_\\lambda(x_0, x)\\neq 0$, thus  we do not have a singularity in the denominator of the fraction. Therefore, the Gaussian kernel will also be differentiable in $x_{0}$, and as consequence, the Nadaraya-Watson kernel-weighted average will also be differentiable.\n",
    "\n",
    "\n",
    "b) Regarding the Epanechnikov kernel with adaptive nearest-neighbor bandwidth $\\lambda(x_0)$:\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using mathematical expressions and reasoning, we can show that the Epanechnikov kernel is not differentiable everywhere. Let $K_{\\lambda}(x_{0},x) = D(|x-x_{0}|/\\lambda)$, where $D(t)$ is defined as:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& K_{\\lambda}\\left(x_{0}, x\\right)=D\\left(\\frac{\\left|x-x_{0}\\right|}{\\lambda}\\right) \\text { with } \\\\\n",
    "& D(t)=\\left\\{\\begin{array}{cc}\n",
    "\\frac{3}{4}\\left(1-t^{2}\\right) & |t| \\leq 1 \\\\\n",
    "0 & \\text { otherwise }\n",
    "\\end{array} .\\right.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "From this expression we will argue that the Epanechnikov kernel is not everywhere differentiable. As $t=\\frac{\\left|x-x_{0}\\right|}{\\lambda}$ when $|t| \\rightarrow 1^{-}$ i.e. for $\\left|x-x_{0}\\right| \\rightarrow \\lambda^{-}$ that is we approach both limits from below we have\n",
    "\n",
    "$$\n",
    "\\frac{d}{d x_{0}} K_{\\lambda}\\left(x_{0}, x\\right)=\\frac{d D(t)}{d t} \\frac{d t}{d x_{0}}=\\left(-\\frac{3}{2} t\\right)\\left(-\\frac{1}{\\lambda}\\right)=\\frac{3}{2} \\frac{t}{\\lambda}=\\frac{3}{2} \\frac{\\left|x-x_{0}\\right|}{\\lambda^{2}} \\neq 0\n",
    "$$\n",
    "\n",
    "In contrast, when $|t| \\rightarrow 1^{+}$ i.e. for $\\left|x-x_{0}\\right| \\rightarrow \\lambda^{+}$ that is we approach both limits from above we have\n",
    "\n",
    "$$\n",
    "\\frac{d}{d x_{0}} K_{\\lambda}\\left(x_{0}, x\\right)=0\n",
    "$$\n",
    "\n",
    "because the function is identically zero in that domain. As these two limits are not equal the Epanechnikov kernel is not differentiable everywhere.\n",
    "\n",
    "This last result might have been more easily argued by just plotting the function $D(t)$ and observing that the function has at least a point at $|t|=1$  where it is not differentiable.\n",
    "\n",
    "This problem is still there even in the case where we have an adaptive nearest-neighbor bandwidth $\\lambda\\left(x_{0}\\right)$ since the discontinuity will still be there whenever\n",
    "\n",
    "$$\n",
    "\\frac{\\left|x-x_{0}\\right|}{\\lambda\\left(x_{0}\\right)}=1\n",
    "$$\n",
    "\n",
    "Depending on the function $\\lambda\\left(x_{0}\\right)$ (i.e. whether it is has a very small or large magnitude) this problem is worse/better than with a constant $\\lambda$. Therefore, $\\lambda$ kind of serves as a parameter for controlling this effect.\n",
    "\n",
    "For $k$-nearest neighbor methods an additional problem is that\n",
    "\n",
    "$$\n",
    "\\lambda\\left(x_{0}\\right)=h_{k}\\left(x_{0}\\right)=\\left|x_{0}-x_{[k]}\\right|\n",
    "$$\n",
    "\n",
    "is not differentiable. To see this, take $2 k+1$ consecutive $x_{k}$ 's such that\n",
    "\n",
    "$$\n",
    "x_{1}<x_{2}<\\cdots<x_{2 k}<x_{2 k+1}\n",
    "$$\n",
    "\n",
    "Let us evaluate the derivative of $h_{k}(x)$ at $x_{m}=\\frac{x_{1}+x_{2 k+1}}{2}$ i.e. the midpoint of the range of the $x$. Now\n",
    "\n",
    "$$\n",
    "h_{k}\\left(x_{m}^{-}\\right)=\\left|x_{m}-x_{1}\\right|=x_{m}-x_{1}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\lim _{x_{0} \\rightarrow x_{m}} \\frac{d}{d x_{0}} h_{k}\\left(x_{0}\\right)=+1\n",
    "$$\n",
    "\n",
    "$$\n",
    "h_{k}\\left(x_{m}^{+}\\right)=\\left|x_{m}-x_{2 k+1}\\right|=x_{2 k+1}-x_{m}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\lim _{x_{0} \\rightarrow x_{m}} \\frac{d}{d x_{0}} h_{k}\\left(x_{0}\\right)=-1\n",
    "$$\n",
    "\n",
    "As $+1 \\neq-1$ the derivative of $h_{k}\\left(x_{0}\\right)$ at $x_{0}=x_{m}$ has a discontinuity there.\n",
    "\n",
    "Interestingly, the tri-cube kernel defined as follows:\n",
    "\n",
    "$$\n",
    "D(t)=\\left\\{\\begin{array}{cc}\n",
    "\\left(1-|t|^{3}\\right)^{3} & |t| \\leq 1 \\\\\n",
    "0 & \\text { otherwise }\n",
    "\\end{array}\\right.\n",
    "$$\n",
    "\n",
    "has not this problem at $|t|=1$ and will be differentiable in that point:\n",
    "\n",
    "$$\n",
    "D^{\\prime}(t)=3\\left(1-|t|^{3}\\right)^{2}\\left(\\frac{d}{d t}\\left(-|t|^{3}\\right)\\right) .\n",
    "$$\n",
    "\n",
    "It does not matter how to \"evaluate\" the $t$ derivative of $|t|^{3}$ in the above expression since the other factor $1-|t|^{3}=0$ when $|t|=1$ and so we have $D^{\\prime}(t)=0$ there.\n",
    "\n",
    "Therefore, we can say that $D(t)$ is continuous for all $t$, but not differentiable at $t=1$. Thus, the same will occur with the Epanechnikov kernel.\n",
    "\n",
    "Furthermore, even in the case of an adaptive nearest-neighbor bandwidth $\\lambda(x_0)$, where $\\frac{|x-x_0|}{\\lambda(x_0)}$ approaches 1 from different directions, $\\hat f(x_0)$ will still not be differentiable owing to the same reasons when $\\frac{|x-x_0|}{\\lambda(x_0)}$ approaches 1 coming from different directions.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 5 Ex. 6.2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ex. 6.2: Show that $\\sum_{i=1}^n (x_i-x_0)l_i(x_0) = 0$ for local linear regression. Define $b_j(x_0) = \\sum_{i=1}^n (x_i - x_0)^j l_i(x_0)$. Show that $b_0(x_0) = 1$ for local polynomial regression of any degree (including local constants). Show that $b_j(x_0) = 0$ for all $j \\in {1, 2, \\dots, k}$ for local polynomial regression of degree $k$. What are the implications of this on the bias?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given $B$ as the $N\\times (k+1)$ matrix with all its $i$-th rows $b(x_i)^T$, where $b(x)^T = (1,x,x^2,\\dots,x^k)$ for $k\\geq 0$, and $W(x_0)$ as the $N \\times N$ diagonal matrix with its $i$-th diagonal element $K_\\lambda(x_0, x_i)$, we can obtain the following equations:\n",
    "\n",
    "\\begin{equation}\n",
    "b(x_0)^T = b(x_0)^T(\\mathbf{B}^T\\mathbf{W}(x_0)\\mathbf{B})^{-1}\\mathbf{B}^T\\mathbf{W}(x_0)\\mathbf{B}.\n",
    "\\end{equation}\n",
    "\n",
    "Combining the equation that defines $l_i(x_0)$ and the above equation, we get:\n",
    "\n",
    "\\begin{align}\n",
    "1 &= b(x_0)^T(\\mathbf{B}^T\\mathbf{W}(x_0)\\mathbf{B})^{-1}\\mathbf{B}^T\\mathbf{W}(x_0)\\mathbf{1} = \\sum_{i=1}^Nl_i(x_0) \\nonumber\\\n",
    "x_0 &= b(x_0)^T(\\mathbf{B}^T\\mathbf{W}(x_0)\\mathbf{B})^{-1}\\mathbf{B}^T\\mathbf{W}(x_0)\\mathbf{B}2 = \\sum{i=1}^Nl_i(x_0)x_i \\nonumber\\\n",
    "\\cdots \\nonumber\\\n",
    "x_0^k &=b(x_0)^T(\\mathbf{B}^T\\mathbf{W}(x_0)\\mathbf{B})^{-1}\\mathbf{B}^T\\mathbf{W}(x_0)\\mathbf{B}{k+1} = \\sum{i=1}^Nl_i(x_0)x_i^k \\nonumber\n",
    "\\end{align}\n",
    "\n",
    "where $B_i$ is the $i$-th column of $B$ and $B_1 = 1$.  \n",
    "\n",
    "Then, \n",
    "\n",
    "$b_0(x_0) = \\sum_{i=1}^Nl_i(x_0) = 1$  \n",
    "\n",
    "and  \n",
    "\n",
    "$b_1(x_0) = \\sum_{i=1}^N(x_i-x_0)l_i(x_0) = \\sum_{i=1}^Nl_i(x_0)x_i - x_0\\sum_{i=1}^Nl_i(x_0) = x_0 - x_0\\cdot 1 = 0$ \n",
    "\n",
    "\n",
    "//////////\n",
    "\n",
    "for local polynomial regression of degree $k$. This implies that the bias term of the local polynomial regression of degree $k$ is zero, which means that the fitted curve passes exactly through the average of the data at each point $x_i$.\n",
    "\n",
    "////////////////////\n",
    "\n",
    "When $j\\ge 2$\n",
    "\n",
    "\\begin{align}\n",
    "b_j(x_0) &= \\sum_{i=1}^N(x_i-x_0)^jl_i(x_0)\\\n",
    "&=\\sum_{i=1}^N\\left(\\sum_{b=0}^j\\binom{j}{b}(-1)^bx_i^{j-b}x_0^b\\right)l_i(x_0)\\\n",
    "&=\\sum_{b=0}^j\\binom{j}{b}(-1)^bx_0^b\\left(\\sum_{i=1}^Nl_i(x_0)x_i^{j-b}\\right)\\\n",
    "&=\\sum_{b=0}^j\\binom{j}{b}(-1)^bx_0^bx_0^{j-b}\\\n",
    "&=\\sum_{b=0}^j\\binom{j}{b}(-1)^bx_0^j\\\n",
    "&=0\\quad \\text{for }j\\geq 2\n",
    "\\end{align}\n",
    "\n",
    "Applying the Taylor expansion:\n",
    "\n",
    "\\begin{align}\n",
    "E[\\hat f(x_0)] - f(x_0) &= \\sum_{i=1}^Nl_i(x_0)f(x_i) - f(x_0)\\\n",
    "&= f(x_0)\\sum_{i=1}^Nl_i(x_0) - f(x_0) + f'(x_0)\\sum_{i=1}^N(x_i-x_0)l_i(x_0)\\\n",
    "&+ \\frac{f''(x_0)}{2}\\sum_{i=1}^N(x_i-x_0)^2l_i(x_0)\\\n",
    "&+ \\cdots\\\n",
    "&+ (-1)^k\\frac{f^{(k)}}{k!}\\sum_{i=1}^N(x_i-x_0)^kl_i(x_0)\\\n",
    "&+ R\\\n",
    "&= R\n",
    "\\end{align}\n",
    "\n",
    "Finally, we can appreciate that the bias of the approximation or model will be determined by the high-order derivatives of the real function $f$ that are present in the remaining term $R$. Hence, the bias will not depend on the specific coefficients or parameters of the model, but rather on the underlying function being approximated and the complexity of the model.\n",
    "\n",
    "In other words, the bias is determined by the fundamental characteristics of the function being approximated, and not by the particular algorithm or model used to perform the approximation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LASTTTTTTT  \n",
    "\n",
    "For local polynomial regression of degree $k$ let $B$ be the $N \\times(k+1)$ matrix given by\n",
    "\n",
    "$$\n",
    "B=\\left[\\begin{array}{cccc}\n",
    "1 & x_{1} & \\ldots & x_{1}^{k} \\\\\n",
    "1 & x_{2} & \\ldots & x_{2}^{k} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "1 & x_{N} & \\ldots & x_{N}^{k}\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "This has its $i$ th row given by the vector valued function $b(x)$ where\n",
    "\n",
    "$$\n",
    "b(x)^{T}=\\left(1, x, \\ldots, x^{k-1}, x^{k}\\right)\n",
    "$$\n",
    "\n",
    "Finally, let $W\\left(x_{0}\\right)$ be the $N \\times N$ diagonal matrix with the $i$ th diagonal element $K_{\\lambda}\\left(x_{0}, x_{i}\\right)$. Then based on the results from the book the local polynomial expansion at a point $x_{0}$ can be written as\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\hat{f}\\left(x_{0}\\right) & =b\\left(x_{0}\\right)^{T}\\left(B^{T} W\\left(x_{0}\\right) B\\right)^{-1} B^{T} W\\left(x_{0}\\right) \\mathbf{y} \\\\\n",
    "& =\\sum_{i=1}^{N} l_{i}\\left(x_{0}\\right) y_{i}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Based on Equation 123 we will replace the vector $\\mathbf{y}$ with a vector $\\mathbf{v}_{j}$ that is the $j$ th power of each of the scalar $x$ s in the training set i.e. with the vector\n",
    "\n",
    "$$\n",
    "\\mathbf{v}_{j}^{T}=\\left(x_{1}^{j}, x_{2}^{j}, \\cdots, x_{N-1}^{j}, x_{N}^{j}\\right)\n",
    "$$\n",
    "\n",
    "for $j=0,1,2, \\ldots, k-1, k$. Using Equations 123 and 124 for a fixed $j$ this is\n",
    "\n",
    "$$\n",
    "b\\left(x_{0}\\right)^{T}\\left(B^{T} W\\left(x_{0}\\right) B\\right)^{-1} B^{T} W\\left(x_{0}\\right) \\mathbf{v}_{j}=\\sum_{i=1}^{N} l_{i}\\left(x_{0}\\right) x_{i}^{j}\n",
    "$$\n",
    "\n",
    "If we repeat the above equation for each $j$ i.e. for $0 \\leq j \\leq k$ then since the matrix $B$ is the column concatenation of the $\\mathbf{v}_{j}$ s i.e. since $B=\\left[\\mathbf{v}_{0}, \\mathbf{v}_{1}, \\ldots, \\mathbf{v}_{k-1}, \\mathbf{v}_{k}\\right]$ when we write all $k+1$ of these expressions in matrix form the left-hand-side of this will be\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "b\\left(x_{0}\\right)^{T}\\left(B^{T} W\\left(x_{0}\\right) B\\right)^{-1} B^{T} W\\left(x_{0}\\right)\\left[\\mathbf{v}_{0}, \\mathbf{v}_{i}, \\ldots, \\mathbf{v}_{k-1}, \\mathbf{v}_{k}\\right] & =b\\left(x_{0}\\right)^{T}\\left(B^{T} W\\left(x_{0}\\right) B\\right)^{-1} B^{T} W\\left(x_{0}\\right) B \\\\\n",
    "& =b\\left(x_{0}\\right)^{T} \\\\\n",
    "& =\\left[1, x_{0}, x_{0}^{2}, \\ldots, x_{0}^{k}\\right],\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "which is a row vector. The right-hand-side of that procedure will be the row-vector\n",
    "\n",
    "$$\n",
    "\\left[\\sum_{i=1}^{N} l_{i}\\left(x_{0}\\right), \\sum_{i=1}^{N} l_{i}\\left(x_{0}\\right) x_{i}, \\ldots, \\sum_{i=1}^{N} l_{i}\\left(x_{0}\\right) x_{i}^{k}\\right]\n",
    "$$\n",
    "\n",
    "If we compare the components of these two vectors we see that\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{N} l_{i}\\left(x_{0}\\right) x_{i}^{j}=x_{0}^{j}\n",
    "$$\n",
    "\n",
    "for $0 \\leq j \\leq k$. Specifically taking $j=0$ and $j=1$ these are\n",
    "\n",
    "$$\n",
    "\\begin{gathered}\n",
    "\\sum_{i=1}^{N} l_{i}\\left(x_{0}\\right)=1 \\\\\n",
    "\\sum_{i=1}^{N} l_{i}\\left(x_{0}\\right) x_{i}=x_{0}\n",
    "\\end{gathered}\n",
    "$$\n",
    "\n",
    "Multiplying the first of these by $x_{0}$ and subtracting it from the second gives\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{N} l_{i}\\left(x_{0}\\right)\\left(x_{i}-x_{0}\\right)=0\n",
    "$$\n",
    "\n",
    "one of the desired expressions.\n",
    "\n",
    "We now define $b_{j}\\left(x_{0}\\right)$ as\n",
    "\n",
    "$$\n",
    "b_{j}\\left(x_{0}\\right) \\equiv \\sum_{i=1}^{N}\\left(x_{i}-x_{0}\\right)^{j} l_{i}\\left(x_{0}\\right)\n",
    "$$\n",
    "\n",
    "From Equation 126 we see that $b_{0}\\left(x_{0}\\right)=1$. From Equation 128 we see that $b_{1}\\left(x_{0}\\right)=0$. For $j>1$ using the binomial theorem twice and Equation 125 we have\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "b_{j}\\left(x_{0}\\right) & =\\sum_{i=1}^{N}\\left(x_{i}-x_{0}\\right)^{j} l_{i}\\left(x_{0}\\right) \\\\\n",
    "& =\\sum_{i=1}^{N} \\sum_{k=0}^{j}\\left(\\begin{array}{l}\n",
    "j \\\\\n",
    "k\n",
    "\\end{array}\\right) x_{i}^{k}\\left(-x_{0}\\right)^{j-k} l_{i}\\left(x_{0}\\right) \\\\\n",
    "& =\\sum_{k=0}^{j}\\left(\\begin{array}{l}\n",
    "j \\\\\n",
    "k\n",
    "\\end{array}\\right)\\left(-x_{0}\\right)^{j-k} \\sum_{i=1}^{N} x_{i}^{k} l_{i}\\left(x_{0}\\right) \\\\\n",
    "& =\\sum_{k=0}^{j}\\left(\\begin{array}{l}\n",
    "j \\\\\n",
    "k\n",
    "\\end{array}\\right)\\left(-x_{0}\\right)^{j-k} x_{0}^{k}=\\left(x_{0}-x_{0}\\right)^{j}=0 .\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The implications of this on the bias are that if the true form of $Y$ has a polynomial form (in $x)$ then these models will be unbiased (see the books Eq. 6.10).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
