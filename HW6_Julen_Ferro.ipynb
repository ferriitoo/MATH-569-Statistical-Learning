{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PROBLEM 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Ex. 8.1}$  \n",
    "Let $r(y)$ and $q(y)$ be probability density functions. Jensen's inequality states that for a random variable $X$ and a convex function $\\phi(x)$, $\\mathbb{E}[\\phi(X)] \\geq \\phi[\\mathbb{E}(X)]$. Use Jensen's inequality to show that\n",
    "\\begin{equation}\n",
    "\\mathbb{E}_q \\log \\frac{r(Y)}{q(Y)} \\tag{8.61}\n",
    "\\end{equation}\n",
    "is maximized as a function of $r(y)$ when $r(y) = q(y)$. Hence show that $R(\\theta, \\theta) \\geq R(\\theta', \\theta)$ as stated below equation (8.46).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PROBLEM 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Ex. 8.3}$   \n",
    "Justify the estimate (8.50), using the relationship\n",
    "\\begin{equation}\n",
    "\\text{Pr}(A) = \\int \\text{Pr}(A|B)d(\\text{Pr}(B)).\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PROBLEM 3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$\\textbf{Ex. 8.4}$  \n",
    "Consider the bagging method of Section 8.7. Let our estimate $\\hat{f}(x)$ be the B-spline smoother $\\hat{\\mu}(x)$ of Section 8.2.1. Consider the parametric bootstrap of equation (8.6), applied to this estimator. Show that if we bag $\\hat{f}(x)$, using the parametric bootstrap to generate the bootstrap samples, the bagging estimate $\\hat{f}_{\\text{bag}}(x)$ converges to the original estimate $\\hat{f}(x)$ as $B \\rightarrow \\infty$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PROBLEM 4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Ex. 11.2}$  \n",
    "Consider a neural network for a quantitative outcome as in (11.5), using squared-error loss and identity output function $g_k(t) = t$. Suppose that the weights $\\alpha_m$ from the input to hidden layer are nearly zero. Show that the resulting model is nearly linear in the inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PROBLEM 5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Ex. 11.3}$   \n",
    "Derive the forward and backward propagation equations for the cross-entropy loss function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PROBLEM 6"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a program to implement the neural network model of Net-2 in figure 11.10, and fit the neural network to the zipcode data. Compare the results to\n",
    "logistic regression on classification performance. The zipcode data are provided. Due to the large size of the dataset, you are only considering the classification of three digits, 0, 1, and 2. Use a 70-30 split for training and testing data.  \n",
    "\n",
    "Using library functions is acceptable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
