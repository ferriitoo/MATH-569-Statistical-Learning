{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PROBLEM 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**$\\textbf{Ex. 8.1}$**    \n",
    "**Let $r(y)$ and $q(y)$ be probability density functions. Jensen's inequality states that for a random variable $X$ and a convex function $\\phi(x)$, $\\mathbb{E}[\\phi(X)] \\geq \\phi[\\mathbb{E}(X)]$. Use Jensen's inequality to show that**\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbb{E}_q \\log \\frac{r(Y)}{q(Y)} \\tag{8.61}\n",
    "\\end{equation}\n",
    "\n",
    "**is maximized as a function of $r(y)$ when $r(y) = q(y)$. Hence show that $R(\\theta, \\theta) \\geq R(\\theta', \\theta)$ as stated below equation (8.46).**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider that $-\\log(x)$ is a convex function. By applying Jensen's inequality, we obtain:\n",
    "\n",
    "\\begin{equation}\n",
    "E_q[-\\log[r(Y)/q(Y)]] \\ge -\\log[E_q[r(Y)/q(Y)]] \\\n",
    "= -\\log\\left[\\int\\frac{r(y)}{q(y)}q(y)dy\\right] \\\n",
    "= -\\log\\left[\\int r(y)dy\\right] \\\n",
    "= -\\log(1) \\\n",
    "= 0,\n",
    "\\end{equation}\n",
    "\n",
    "which implies that\n",
    "\n",
    "\\begin{equation}\n",
    "E_q[\\log(r(Y)/q(Y))] \\le 0 = E_q[\\log(q(Y)/q(Y))].\n",
    "\\end{equation}\n",
    "\n",
    "The expectation is maximized when $r = q$, indicating that the model is best when the true distribution matches the modeled distribution.\n",
    "\n",
    "For equation (8.46) in the text, we have:\n",
    "\n",
    "\\begin{equation}\n",
    "R(\\theta', \\theta) - R(\\theta, \\theta) = E[\\ell_1(\\theta;\\mathbb{Z}^m|\\mathbb{Z})|\\mathbb{Z}, \\theta] - E[\\ell(\\theta;\\mathbb{Z}^m|\\mathbb{Z})|\\mathbb{Z}, \\theta] \\\n",
    "= E_{\\text{Pr}(\\mathbb{Z}^m|\\mathbb{Z}, \\theta)}\\left(\\log \\frac{\\text{Pr}(\\mathbb{Z}^m|\\mathbb{Z}, \\theta')}{\\text{Pr}(\\mathbb{Z}^m|\\mathbb{Z}, \\theta)}\\right) \\\n",
    "\\le0.\n",
    "\\end{equation}\n",
    "\n",
    "This inequality shows that the difference in risk values between the two parameter settings, $\\theta$ and $\\theta'$, is less than or equal to 0. This result is useful for comparing different model configurations and indicates that the risk is minimized when the model parameters match the true underlying distribution."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other words, we can demonstrate the inequality:\n",
    "\n",
    "\\begin{equation}\n",
    "\\ln(1 + x) \\leq x,\n",
    "\\end{equation}\n",
    "\n",
    "which holds for any $x$. This inequality is explicitly true when $x = 0$. By substituting $\\frac{r(y)}{g(y)} - 1$ for $x$, we obtain the following inequality:\n",
    "\n",
    "\\begin{equation}\n",
    "\\ln \\left( \\frac{r(y)}{g(y)} \\right) \\leq \\frac{r(y) - g(y)}{g(y)}.\n",
    "\\end{equation}\n",
    "\n",
    "Since the function $g(y)$ is non-negative for any $y$, we can derive the following inequality for the integrals:\n",
    "\n",
    "\\begin{equation}\n",
    "\\int \\left( \\frac{r(y)}{g(y)} \\right) g(y) dy \\leq \\int \\frac{r(y) - g(y)}{g(y)} g(y) dy = \\int r(y) dy - \\int g(y) dy = 1 - 1 = 0.\n",
    "\\end{equation}\n",
    "\n",
    "Now, let's plug in the probability values for $r$ and $q$, as well as the difference in risk values, as follows:\n",
    "\n",
    "\n",
    "$r = \\Pr(Z_m | Z, \\theta')$   \n",
    "$q = \\Pr(Z_m | Z, \\theta)$  \n",
    "$R(\\theta') - R(\\theta)$  \n",
    "\n",
    "Substituting these values, we get:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbb{E}_{Z_m | Z, \\theta} \\ln \\left( \\frac{\\Pr(Z_m | Z, \\theta')}{\\Pr(Z_m | Z, \\theta)} \\right) = \\mathbb{E}_q \\ln \\left( \\frac{r}{q} \\right) \\leq 0.\n",
    "\\end{equation}\n",
    "\n",
    "As before mentioned, this result indicates that the expected value of the logarithm of the ratio of probabilities under the true distribution $\\Pr(Z_m | Z, \\theta)$ is less than or equal to 0. The inequality provides a bound on the difference between the risk values for the two parameter settings, $\\theta$ and $\\theta'$, which is a useful result for comparing different model configurations."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PROBLEM 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**$\\textbf{Ex. 8.3}$**   \n",
    "\n",
    "**Justify the estimate (8.50), using the relationship**\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{Pr}(A) = \\int \\text{Pr}(A|B)d(\\text{Pr}(B)).\n",
    "\\end{equation}\n",
    "\n",
    "**Equation 8.50:**    \n",
    "\\begin{equation}\n",
    "    \\frac{1}{M-m+1}\\sum_{t=m}^M\\text{Pr}\\left(u|U_l^{(t)}, l\\neq k\\right).\n",
    "\\end{equation}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To justify the estimate given by equation (8.50), let's first understand the context of the problem. We are given a statistical learning problem where we have training data ${U_l^{(t)}, l \\neq k}$ with $l$ representing the class labels and $t$ representing the data points. We are interested in estimating the probability $\\text{Pr}(u|U_l^{(t)}, l \\neq k)$ for a given class $u$ and some data point $U_l^{(t)}$.\n",
    "\n",
    "We can use the relationship provided in equation (1) to compute the probability $\\text{Pr}(u|U_l^{(t)}, l \\neq k)$ by marginalizing over the distribution of the data points ${U_l^{(t)}}$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{Pr}(u|U_l^{(t)}, l\\neq k) = \\int \\text{Pr}(u|U_l^{(t)}, B, l\\neq k)d(\\text{Pr}(B)).\n",
    "\\end{equation}\n",
    "\n",
    "Now, let's divide the range of data points into $M - m + 1$ equally spaced intervals, and let the midpoints of these intervals be denoted as $U_l^{(m)}, U_l^{(m+1)}, \\dots, U_l^{(M)}$. We can approximate the integral by a summation over these midpoints using the midpoint rule:\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{Pr}(u|U_l^{(t)}, l\\neq k) \\approx \\frac{1}{M-m+1}\\sum_{t=m}^M\\text{Pr}\\left(u|U_l^{(t)}, B, l\\neq k\\right)d(\\text{Pr}(B)).\n",
    "\\end{equation}\n",
    "\n",
    "Now, as the summand does not depend on the measure $d(\\text{Pr}(B))$, we can remove it from the sum:\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{Pr}(u|U_l^{(t)}, l\\neq k) \\approx \\frac{1}{M-m+1}\\sum_{t=m}^M\\text{Pr}\\left(u|U_l^{(t)}, l\\neq k\\right).\n",
    "\\end{equation}\n",
    "\n",
    "This justifies the estimate given by equation (8.50).\n",
    "\n",
    "Equation 8.50:  \n",
    "\\begin{equation}\n",
    "    \\frac{1}{M-m+1}\\sum_{t=m}^M\\text{Pr}\\left(u|U_l^{(t)}, l\\neq k\\right).\n",
    "\\end{equation}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PROBLEM 3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**$\\textbf{Ex. 8.4}$**    \n",
    "**Consider the bagging method of Section 8.7. Let our estimate $\\hat{f}(x)$ be the B-spline smoother $\\hat{\\mu}(x)$ of Section 8.2.1. Consider the parametric bootstrap of equation (8.6), applied to this estimator. Show that if we bag $\\hat{f}(x)$, using the parametric bootstrap to generate the bootstrap samples, the bagging estimate $\\hat{f}_{\\text{bag}}(x)$ converges to the original estimate $\\hat{f}(x)$ as $B \\rightarrow \\infty$.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When employing bagging, the bagged estimate is defined as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat f_{\\text{bag}}(x) = \\frac{1}{B}\\sum_{b=1}^B\\hat f^\\ast_b(x),\n",
    "\\end{equation}\n",
    "\n",
    "where each individual model is expressed as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat f^\\ast_b(x) = S\\mathbf{y}^\\ast = S(\\hat f(x) + \\epsilon_b).\n",
    "\\end{equation}\n",
    "\n",
    "For a B-spline smoother, the smoothing matrix $S$ is given by:\n",
    "\n",
    "\\begin{equation}\n",
    "S = N(N^TN)^{-1}N^T.\n",
    "\\end{equation}\n",
    "\n",
    "This matrix $S$ depends on the input data $x$ rather than the response vector $\\mathbf{y}$.\n",
    "\n",
    "To perform parametric bootstrap, we generate $n$ new response samples $\\mathbf{y}^*$ by adding noise to the fitted values:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{y}^* = \\hat{f}(x) + \\boldsymbol{\\epsilon}.\n",
    "\\end{equation}\n",
    "\n",
    "We repeat this process $B$ times to obtain a new vector $\\mathbf{y}^*$ for each of the $B$ bagging iterations.\n",
    "\n",
    "The noise $\\epsilon_b$ follows a normal distribution with mean 0 and variance $\\sigma^2$ in the case of B-spline smoothers. Using this information, the fitted model for each bootstrap sample can be written as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{f}^(x) = S\\mathbf{y}^ = S(\\hat{f}(x) + \\boldsymbol{\\epsilon}) = S^2 \\mathbf{y} + S\\boldsymbol{\\epsilon} = S\\mathbf{y} + S\\boldsymbol{\\epsilon}.\n",
    "\\end{equation}\n",
    "\n",
    "Here, we utilize the idempotent property of $S$, where $S^2 = S$. We can denote the fitted model for the $j$-th bootstrap as $\\hat{f}^*_j$ and the corresponding noise as $\\boldsymbol{\\epsilon}_j$.\n",
    "\n",
    "By aggregating $B$ simulated samples, we obtain the bagged estimate at $x$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{f}{\\text{bag}}(x) = \\frac{1}{B}\\sum{j=1}^B \\hat{f}^*j(x) = S\\mathbf{y} + S\\left(\\frac{1}{B}\\sum{j=1}^B \\boldsymbol{\\epsilon}_j\\right).\n",
    "\\end{equation}\n",
    "\n",
    "As the number of bagging iterations $B$ approaches infinity, the average noise term converges to zero:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{1}{B}\\sum_{j=1}^B \\boldsymbol{\\epsilon}_j \\to 0,\n",
    "\\end{equation}\n",
    "\n",
    "Thus, the bagged estimate converges to the original fitted model:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{f}_{\\text{bag}}(x) = S\\mathbf{y} = \\hat{f}(x),\n",
    "\\end{equation}\n",
    "\n",
    "as $B \\to \\infty$. This result demonstrates the convergence of the bagged estimator to the original fitted model when the number of bootstrap samples increases indefinitely."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PROBLEM 4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**$\\textbf{Ex. 11.2}$**    \n",
    "**Consider a neural network for a quantitative outcome as in (11.5), using squared-error loss and identity output function $g_k(t) = t$. Suppose that the weights $\\alpha_m$ from the input to hidden layer are nearly zero. Show that the resulting model is nearly linear in the inputs.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this exercise is to investigate the behavior of a neural network using the sigmoid activation function, or more generally, any activation function with the property of being nearly linear around the origin. We will take for granted in this exercise that the neural network uses sigmoid activation function (actually, any general activation function with the property that it's almost linear near origin). Let's first recall the sigmoid activation function:\n",
    "\n",
    "\\begin{equation}\n",
    "\\sigma(\\nu) = \\frac{1}{1 + e^{-\\nu}}.\n",
    "\\end{equation}\n",
    "\n",
    "Now, let's examine the Taylor expansion for the activation function $Ïƒ(v)$ in the context of the neural network:\n",
    "\n",
    "\\begin{align*}\n",
    "\\sigma(\\alpha_0^m + \\alpha^T_m \\mathbf{X}) &= \\frac{1}{1 + \\exp(-\\alpha_0^m - \\alpha^T_m \\mathbf{X})}\\\n",
    "&= \\frac{\\exp(\\alpha_0^m)}{\\exp(\\alpha_0^m) + \\exp(-\\alpha^T_m \\mathbf{X})}\\\n",
    "&= \\left(\\frac{\\exp(\\alpha_0^m)}{1 + \\epsilon(\\alpha_0^m)}\\right) \\left(\\frac{1}{\\frac{\\exp(-\\alpha^T_m \\mathbf{X}) - 1}{1 + \\exp(\\alpha_0^m) + 1}}\\right)\n",
    "\\end{align*}\n",
    "\n",
    "We can analyze the behavior of the activation function as $\\mathbf{X}$ approaches zero:\n",
    "\n",
    "\\begin{equation}\n",
    "\\exp(-\\alpha^T_m \\mathbf{X}) - 1 \\to -\\alpha^T_m \\mathbf{X},\n",
    "\\end{equation}\n",
    "\n",
    "which leads to:\n",
    "\n",
    "\\begin{equation}\n",
    "\\sigma(\\alpha_0^m + \\alpha^T_m \\mathbf{X}) \\to \\left(\\frac{\\exp(\\alpha_0^m)}{1 + \\epsilon(\\alpha_0^m)}\\right) \\left(1 + \\frac{\\alpha^T_m \\mathbf{X}}{1 + \\exp(\\alpha_0^m)}\\right).\n",
    "\\end{equation}\n",
    "\n",
    "From this, it becomes clear that the function $f(\\mathbf{X})$ is essentially a linear function of $\\mathbf{X}$.\n",
    "\n",
    "Now, let's define $\\mu(x) = \\frac{1}{2}(1+x)$. We find that:\n",
    "\n",
    "\\begin{equation}\n",
    "\\lim_{x \\to 0} \\frac{\\mu(x)}{\\sigma(x)} = \\lim_{x \\to 0} \\frac{1}{2}(1+x)(1+e^{-x})\\\n",
    "= \\lim_{x \\to 0} \\frac{1}{2}\\left(1 + e^{-x} + x + xe^{-x}\\right)\\\n",
    "= 1.\n",
    "\\end{equation}\n",
    "\n",
    "Considering the case when $\\alpha_m$ values are close to zero, the values of $\\alpha_m^T\\mathbf{X}$ will also be small. If we incorporate a bias term and expand $x$ (i.e., each row of $x$ has a 1 in the first position), we obtain $Z_m = \\sigma(\\alpha_m^T\\mathbf{X}) \\approx \\frac{1}{2}(1 + \\alpha_m^T\\mathbf{X})$. As a result, the model exhibits a nearly linear behavior in $x$ since $g_k$ is an identity function.\n",
    "\n",
    "In conclusion, we have demonstrated that when employing a sigmoid activation function or any other activation function exhibiting near-linear properties close to the origin, the neural network model tends to approximate a linear behavior in $x$. This observation is particularly relevant when the $\\alpha_m$ values are close to zero, further emphasizing the linear characteristics of the model in such cases."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PROBLEM 5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**$\\textbf{Ex. 11.3}$**\n",
    "**Derive the forward and backward propagation equations for the cross-entropy loss function.**\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider a neural network with $L$ layers, and let's denote the activation function as $g(\\cdot)$. Let the weights and biases be represented by $W^{[l]}$ and $b^{[l]}$, respectively, for layer $l$. We have input $x$ and output $y$, and the neural network's output is $\\hat{y}$. We will use the cross-entropy loss function, which is defined as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathcal{L}(y, \\hat{y}) = -\\sum_{i} y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i)\n",
    "\\end{equation}\n",
    "\n",
    "Now, let's derive the forward and backward propagation equations.\n",
    "\n",
    "$\\textbf{Forward propagation}$: This is the process of computing the output of the neural network given an input $x$. We calculate the output of each layer based on the output of the previous layer:\n",
    "\n",
    "Initialize $a^{[0]} = x$.\n",
    "For $l = 1, 2, \\dots, L$:\n",
    "a. Calculate $z^{[l]} = W^{[l]} a^{[l-1]} + b^{[l]}$.\n",
    "b. Calculate $a^{[l]} = g^{[l]}(z^{[l]})$.\n",
    "\n",
    "The final output of the network is $\\hat{y} = a^{[L]}$.\n",
    "\n",
    "$\\textbf{Backward propagation}$: This is the process of computing the gradients of the loss function with respect to the weights and biases of the neural network. We use the chain rule to compute these gradients:\n",
    "\n",
    "Calculate the error in the output layer: $\\delta^{[L]} = - \\frac{\\partial \\mathcal{L}}{\\partial z^{[L]}} = \\frac{\\partial \\mathcal{L}}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial z^{[L]}} = (a^{[L]} - y) \\odot g'^{[L]}(z^{[L]})$.\n",
    "For $l = L-1, L-2, \\dots, 1$:\n",
    "a. Calculate the error in layer $l$: $\\delta^{[l]} = \\left( W^{[l+1]T} \\delta^{[l+1]} \\right) \\odot g'^{[l]}(z^{[l]})$.\n",
    "For $l = 1, 2, \\dots, L$:\n",
    "a. Calculate the gradients with respect to the weights: $\\frac{\\partial \\mathcal{L}}{\\partial W^{[l]}} = \\delta^{[l]} a^{[l-1]T}$.\n",
    "b. Calculate the gradients with respect to the biases: $\\frac{\\partial \\mathcal{L}}{\\partial b^{[l]}} = \\delta^{[l]}$.\n",
    "\n",
    "Once the gradients are computed, we can update the weights and biases using a suitable optimization algorithm, such as gradient descent or any of its variants"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other words, inthe context of cross-entropy loss (also known as deviance), the objective function is defined as:\n",
    "\n",
    "\\begin{equation}\n",
    "R(\\theta) = -\\sum_{i=1}^N\\sum_{k=1}^Ky_{ik}\\log(f_k(x_i)),\n",
    "\\end{equation}\n",
    "\n",
    "where $y_{ik}$ is the true label of the $i$-th data point for class $k$, and $f_k(x_i)$ is the predicted probability for class $k$ given the input $x_i$. The corresponding classifier for this objective function is given by:\n",
    "\n",
    "\\begin{equation}\n",
    "G(x) = \\underset{k}{\\operatorname{argmax}}f_k(x).\n",
    "\\end{equation}\n",
    "\n",
    "Let $z_{mi} = \\sigma(\\alpha_{0m} + \\alpha_m^Tx_i)$, as defined in equation (11.5) in the text, and let $z_i = (z_{1i}, z_{2i}, ..., z_{Mi})$. Then, we can rewrite the objective function as:\n",
    "\n",
    "\\begin{equation}\n",
    "R(\\theta) = \\sum_{i=1}^NR_i = \\sum_{i=1}^N\\sum_{k=1}^K\\left(-y_{ik}\\log(f_k(x_i))\\right),\n",
    "\\end{equation}\n",
    "\n",
    "The derivatives of the objective function with respect to the model parameters $\\beta_{km}$ and $\\alpha_{ml}$ are given by:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial R_i}{\\partial \\beta_{km}} = -\\frac{y_{ik}}{f_k(x_i)}g'k(\\beta_k^Tz_i)z{mi},\\\n",
    "\\frac{\\partial R_i}{\\partial \\alpha_{ml}} = -\\sum_{k=1}^K\\frac{y_{ik}}{f_k(x_i)}g'k(\\beta_k^Tz_i)\\beta{km}\\sigma'(\\alpha_{0m} + \\alpha_m^Tx_i)x_{il}.\n",
    "\\end{equation}\n",
    "\n",
    "Based on these derivatives, a gradient descent update cana be performed at the $(r+1)$-th iteration as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "\\beta_{km}^{(r+1)} = \\beta_{km}^{(r)} - \\gamma_r\\sum_{i=1}^N\\frac{\\partial R_i}{\\partial \\beta^{(r)}{km}},\\\n",
    "\\alpha{ml}^{(r+1)} = \\alpha_{ml}^{(r)} - \\gamma_r\\sum_{i=1}^N\\frac{\\partial R_i}{\\partial \\alpha^{(r)}_{ml}},\n",
    "\\end{equation}\n",
    "\n",
    "where $\\gamma_r$ is the learning rate.\n",
    "\n",
    "We can rewrite the partial derivatives as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial R_i}{\\partial \\beta_{km}} = \\delta_{ki}z_{mi},\\\n",
    "\\frac{\\partial R_i}{\\partial \\alpha_{ml}} = s_{mi}x_{il}.\n",
    "\\end{equation}\n",
    "\n",
    "From their definitions, we have the back-propagation equations:\n",
    "\n",
    "\\begin{equation}\n",
    "s_{mi} = \\sigma'(\\alpha_{0m}+\\alpha_m^Tx_i)\\sum_{k=1}^K\\beta_{km}\\delta_{ki}.\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "The back-propagation equations help us compute the gradients of the cross-entropy loss with respect to the model parameters $\\beta_{km}$ and $\\alpha_{ml}$. These gradients are essential for updating the model parameters using gradient-based optimization algorithms, such as gradient descent or its variants.\n",
    "\n",
    "In summary, the forward and backward propagation processes are essential for training a neural network using the cross-entropy loss function. Forward propagation calculates the predicted probabilities for each class given an input, while backward propagation computes the gradients of the loss function with respect to the model parameters. These gradients are then used to update the model parameters, iteratively refining the model's predictions and minimizing the cross-entropy loss over the training data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PROBLEM 6"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write a program to implement the neural network model of Net-2 in figure 11.10, and fit the neural network to the zipcode data. Compare the results to logistic regression on classification performance. The zipcode data are provided. Due to the large size of the dataset, you are only considering the classification of three digits, 0, 1, and 2. Use a 70-30 split for training and testing data.**  \n",
    "\n",
    "**Using library functions is acceptable**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Net-1: No hidden layer, equivalent to multinomial logistic regression. From an 16x16 input, to 10 outputs.\n",
    "Net-2: One hidden layer, 12 hidden units fully connected. From an 16x16 input, to 10 outputs, going through a hidden layer of 12 units.\n",
    "\n",
    "Write a program to implement the neural network model of Net-1 and Net-2 in figure 11.10, and fit the neural network to the zipcode data. Compare the results to logistic regression on classification performance. The zipcode data are provided. Due to the large size of the dataset, you are only considering the classification of three digits, 0, 1, and 2. Use a 70-30 split for training and testing data.  \n",
    "\n",
    "Using library functions is acceptable"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEPS TO BE FOLLOWED\n",
    "\n",
    "- Install the libraries and dependencies: tensorflow, numpy, sklearn\n",
    "- Then, create a Python script and import the required libraries\n",
    "- Load and preprocess the dataset\n",
    "- Create the Neural Network models for Net-1 and Net-2\n",
    "- Fit the Neural Network models to the training data\n",
    "- Evaluate the Neural Network models on the test data\n",
    "- Fit Logistic Regression to the trainig data\n",
    "- Evaluate the Logistic Regression model on the test data\n",
    "- Compare the models and the plots\n",
    "- Draw some conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape for df_0:  (1194, 256)\n",
      "Shape for df_1:  (1005, 256)\n",
      "Shape for df_2:  (731, 256)\n",
      "/n\n",
      "   0    1    2    3    4      5      6      7      8      9    ...    246   \n",
      "0 -1.0 -1.0 -1.0 -1.0 -1.0 -0.454  0.879 -0.745 -1.000 -1.000  ...  1.000  \\\n",
      "1 -1.0 -1.0 -1.0 -1.0 -1.0 -0.877  0.233  1.000  0.996  0.116  ... -0.041   \n",
      "2 -1.0 -1.0 -1.0 -1.0 -1.0 -0.990  0.019  0.640 -0.553 -0.999  ...  0.363   \n",
      "3 -1.0 -1.0 -1.0 -1.0 -1.0  0.100  0.452 -0.665 -0.358 -0.209  ...  0.865   \n",
      "4 -1.0 -1.0 -1.0 -1.0 -1.0 -1.000 -0.320  0.731 -0.657 -1.000  ... -0.663   \n",
      "\n",
      "     247    248    249    250    251  252  253  254  255  \n",
      "0  1.000  1.000  0.506 -0.174 -0.811 -1.0 -1.0 -1.0   -1  \n",
      "1  0.947  0.988  0.009 -0.931 -1.000 -1.0 -1.0 -1.0   -1  \n",
      "2  1.000  0.820 -0.064 -0.969 -1.000 -1.0 -1.0 -1.0   -1  \n",
      "3  1.000  1.000  0.838  0.085 -0.847 -1.0 -1.0 -1.0   -1  \n",
      "4  0.573  0.702 -0.762 -1.000 -1.000 -1.0 -1.0 -1.0   -1  \n",
      "\n",
      "[5 rows x 256 columns]\n",
      "/n\n",
      "DF_0\n",
      "/n\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1194 entries, 0 to 1193\n",
      "Columns: 256 entries, 0 to 255\n",
      "dtypes: float64(254), int64(2)\n",
      "memory usage: 2.3 MB\n",
      "None\n",
      "DF_1\n",
      "/n\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1005 entries, 0 to 1004\n",
      "Columns: 256 entries, 0 to 255\n",
      "dtypes: float64(154), int64(102)\n",
      "memory usage: 2.0 MB\n",
      "None\n",
      "DF_2\n",
      "/n\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 731 entries, 0 to 730\n",
      "Columns: 256 entries, 0 to 255\n",
      "dtypes: float64(254), int64(2)\n",
      "memory usage: 1.4 MB\n",
      "None\n",
      "/n\n"
     ]
    }
   ],
   "source": [
    "path_0 = 'https://raw.githubusercontent.com/ferriitoo/csv/main/digit0.txt'\n",
    "path_1 = 'https://raw.githubusercontent.com/ferriitoo/csv/main/digit1.txt'\n",
    "path_2 = 'https://raw.githubusercontent.com/ferriitoo/csv/main/digit2.txt'\n",
    "\n",
    "df_0 = pd.read_csv(path_0, header = None)\n",
    "df_1 = pd.read_csv(path_1, header = None)\n",
    "df_2 = pd.read_csv(path_2, header = None)\n",
    "\n",
    "print('Shape for df_0: ', df_0.shape)\n",
    "print('Shape for df_1: ', df_1.shape)\n",
    "print('Shape for df_2: ', df_2.shape)\n",
    "\n",
    "print('/n')\n",
    "print(df_0.head())\n",
    "print('/n')\n",
    "\n",
    "print('DF_0')\n",
    "print('/n')\n",
    "print(df_0.info())\n",
    "\n",
    "print('DF_1')\n",
    "print('/n')\n",
    "print(df_1.info())\n",
    "\n",
    "print('DF_2')\n",
    "print('/n')\n",
    "print(df_2.info())\n",
    "print('/n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow.python'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel_selection\u001b[39;00m \u001b[39mimport\u001b[39;00m train_test_split\n\u001b[0;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlinear_model\u001b[39;00m \u001b[39mimport\u001b[39;00m LogisticRegression\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\__init__.py:37\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msys\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39m_sys\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39m_typing\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtools\u001b[39;00m \u001b[39mimport\u001b[39;00m module_util \u001b[39mas\u001b[39;00m _module_util\n\u001b[0;32m     38\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlazy_loader\u001b[39;00m \u001b[39mimport\u001b[39;00m LazyLoader \u001b[39mas\u001b[39;00m _LazyLoader\n\u001b[0;32m     40\u001b[0m \u001b[39m# Make sure code inside the TensorFlow codebase can use tf2.enabled() at import.\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.python'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "def load_data():\n",
    "    path_0 = 'https://raw.githubusercontent.com/ferriitoo/csv/main/digit0.txt'\n",
    "    path_1 = 'https://raw.githubusercontent.com/ferriitoo/csv/main/digit1.txt'\n",
    "    path_2 = 'https://raw.githubusercontent.com/ferriitoo/csv/main/digit2.txt'\n",
    "\n",
    "    df_0 = pd.read_csv(path_0, header=None)\n",
    "    df_1 = pd.read_csv(path_1, header=None)\n",
    "    df_2 = pd.read_csv(path_2, header=None)\n",
    "\n",
    "    X = pd.concat([df_0, df_1, df_2])\n",
    "    y = np.concatenate([np.zeros(len(df_0)), np.ones(len(df_1)), np.full(len(df_2), 2)])\n",
    "\n",
    "    X = X.values / 255.0  # Normalize the input data\n",
    "    return X, y\n",
    "\n",
    "def preprocess_data(X, y):\n",
    "    X = X.reshape(-1, 16, 16, 1)  # Reshape the input data to a 16x16 format\n",
    "    y = tf.keras.utils.to_categorical(y, num_classes=3)  # One-hot encode the labels\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "X, y = load_data()\n",
    "X_train, X_test, y_train, y_test = preprocess_data(X, y)\n",
    "\n",
    "# Create the neural network models for Net-1 and Net-2\n",
    "def create_net1():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(16, 16, 1)),\n",
    "        tf.keras.layers.Dense(3, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def create_net2():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(16, 16, 1)),\n",
    "        tf.keras.layers.Dense(12, activation='relu'),\n",
    "        tf.keras.layers.Dense(3, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Fit the neural network models to the training data\n",
    "net1 = create_net1()\n",
    "net1.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.1)\n",
    "\n",
    "net2 = create_net2()\n",
    "net2.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Evaluate the neural network models on the test data\n",
    "net1_score = net1.evaluate(X_test, y_test)\n",
    "print(\"Net-1 Test Accuracy: \", net1_score[1])\n",
    "\n",
    "net2_score = net2.evaluate(X_test, y_test)\n",
    "print(\"Net-2 Test Accuracy: \", net2_score[1])\n",
    "\n",
    "# Fit logistic regression to the training data and evaluate on the test data\n",
    "logistic_regression = LogisticRegression()\n",
    "logistic_regression.fit(X_train.reshape(-1, 256), np.argmax(y_train, axis=1))\n",
    "y_pred = logistic_regression.predict(X_test.reshape(-1, 256))\n",
    "\n",
    "print(\"Logistic Regression Test Accuracy: \", logistic_regression.score(X_test.reshape(-1, 256), np.argmax(y_test, axis=1)))\n",
    "\n",
    "# Print the classification report\n",
    "print(\"Classification report for Logistic Regression:\")\n",
    "print(classification_report(np.argmax(y_test, axis=1), y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "def load_data():\n",
    "    path_0 = 'https://raw.githubusercontent.com/ferriitoo/csv/main/digit0.txt'\n",
    "    path_1 = 'https://raw.githubusercontent.com/ferriitoo/csv/main/digit1.txt'\n",
    "    path_2 = 'https://raw.githubusercontent.com/ferriitoo/csv/main/digit2.txt'\n",
    "\n",
    "    df_0 = pd.read_csv(path_0, header=None)\n",
    "    df_1 = pd.read_csv(path_1, header=None)\n",
    "    df_2 = pd.read_csv(path_2, header=None)\n",
    "\n",
    "    X = pd.concat([df_0, df_1, df_2])\n",
    "    y = np.concatenate([np.zeros(len(df_0)), np.ones(len(df_1)), np.full(len(df_2), 2)])\n",
    "\n",
    "    X = X.values / 255.0  # Normalize the input data\n",
    "    return X, y\n",
    "\n",
    "def preprocess_data(X, y):\n",
    "    X = X.reshape(-1, 16, 16, 1)  # Reshape the input data to a 16x16 format\n",
    "    y = tf.keras.utils.to_categorical(y, num_classes=3)  # One-hot encode the labels\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "X, y = load_data()\n",
    "X_train, X_test, y_train, y_test = preprocess_data(X, y)\n",
    "\n",
    "# Create the neural network models for Net-1 and Net-2\n",
    "def create_net1():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(16, 16, 1)),\n",
    "        tf.keras.layers.Dense(3, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def create_net2():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(16, 16, 1)),\n",
    "        tf.keras.layers.Dense(12, activation='relu'),\n",
    "        tf.keras.layers.Dense(3, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Fit the neural network models to the training data\n",
    "\n",
    "epchs_1 = 500\n",
    "\n",
    "epchs_2 = 100\n",
    "\n",
    "net1 = create_net1()\n",
    "history_net1 = net1.fit(X_train, y_train, epochs= epchs_1, batch_size=32, validation_split=0.1)\n",
    "\n",
    "net2 = create_net2()\n",
    "history_net2 = net2.fit(X_train, y_train, epochs= epchs_2, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Plot training accuracy and loss for Net-1 and Net-2\n",
    "def plot_history(history, title):\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "    axs[0].plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    axs[0].plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    axs[0].set_title(title + ' - Accuracy')\n",
    "    axs[0].set_xlabel('Epoch')\n",
    "    axs[0].set_ylabel('Accuracy')\n",
    "    axs[0].legend()\n",
    "\n",
    "    axs[1].plot(history.history['loss'], label='Training Loss')\n",
    "    axs[1].plot(history.history['val_loss'], label='Validation Loss')\n",
    "    axs[1].set_title(title + ' - Loss')\n",
    "    axs[1].set_xlabel('Epoch')\n",
    "    axs[1].set_ylabel('Loss')\n",
    "    axs[1].legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Plot the training accuracy and loss for Net-1 and Net-2\n",
    "plot_history(history_net1, 'Net-1')\n",
    "plot_history(history_net2, 'Net-2')\n",
    "\n",
    "# Evaluate the neural network models\n",
    "net1_test_accuracy = net1.evaluate(X_test, y_test, verbose=0)[1]\n",
    "net2_test_accuracy = net2.evaluate(X_test, y_test, verbose=0)[1]\n",
    "\n",
    "print(\"Net-1 Test Accuracy: \", net1_test_accuracy)\n",
    "print(\"Net-2 Test Accuracy: \", net2_test_accuracy)\n",
    "\n",
    "# Logistic Regression\n",
    "logistic_regression = LogisticRegression(solver='lbfgs', multi_class='multinomial')\n",
    "logistic_regression.fit(X_train.reshape(-1, 256), np.argmax(y_train, axis=1))\n",
    "\n",
    "y_pred = logistic_regression.predict(X_test.reshape(-1, 256))\n",
    "\n",
    "print(\"Logistic Regression Test Accuracy: \", logistic_regression.score(X_test.reshape(-1, 256), np.argmax(y_test, axis=1)))\n",
    "\n",
    "# Print the classification report\n",
    "print(\"Classification report for Logistic Regression:\")\n",
    "print(classification_report(np.argmax(y_test, axis=1), y_pred))\n",
    "\n",
    "print(\"Logistic Regression Test Accuracy: \", logistic_regression.score(X_test.reshape(-1, 256), np.argmax(y_test, axis=1)))\n",
    "\n",
    "# Print the classification report\n",
    "print(\"Classification report for Logistic Regression:\")\n",
    "print(classification_report(np.argmax(y_test, axis=1), y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
