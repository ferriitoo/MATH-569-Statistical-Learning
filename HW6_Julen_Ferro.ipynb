{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.1: SOLUS LIBRO\n",
    "8.3: NO SOLUS LIBRO\n",
    "8.4: SOLUS LIBRO\n",
    "11.2: SOLUS LIBRO\n",
    "11.3: NO SOLUS LIBRO\n",
    "\n",
    "ORAIN KOPIATUKO DUT ESL_SOLUZIOETATIK"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PROBLEM 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**$\\textbf{Ex. 8.1}$**    \n",
    "**Let $r(y)$ and $q(y)$ be probability density functions. Jensen's inequality states that for a random variable $X$ and a convex function $\\phi(x)$, $\\mathbb{E}[\\phi(X)] \\geq \\phi[\\mathbb{E}(X)]$. Use Jensen's inequality to show that**\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbb{E}_q \\log \\frac{r(Y)}{q(Y)} \\tag{8.61}\n",
    "\\end{equation}\n",
    "\n",
    "**is maximized as a function of $r(y)$ when $r(y) = q(y)$. Hence show that $R(\\theta, \\theta) \\geq R(\\theta', \\theta)$ as stated below equation (8.46).**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that $-\\log(x)$ is convex, by Jensen's inequality, we have  \n",
    "\n",
    "\\begin{equation}\n",
    "    E_q[-\\log[r(Y)/q(Y)]] \\ge -\\log[E_q[r(Y)/q(Y)]]\\\\\n",
    "    = -\\log\\left[\\int\\frac{r(y)}{q(y)}q(y)dy\\right]\\\\\n",
    "    = -\\log\\left[\\int r(y)dy\\right]\\\\\n",
    "    = -\\log(1)\\\\\n",
    "    = 0,\n",
    "\\end{equation}\n",
    "\n",
    "therefore we have\n",
    "\n",
    "\\begin{equation}\n",
    "    E_q[\\log(r(Y)/q(Y))]\\le 0 = E_q[\\log(q(Y)/q(Y))].\n",
    "\\end{equation}\n",
    "\n",
    "So the expectation is maximized when $r = q$  \n",
    "\n",
    "\n",
    "For equation (8.46) in the text, we have\n",
    "\n",
    "\\begin{equation}\n",
    "    R(\\theta', \\theta) - R(\\theta, \\theta) = E[\\ell_1(\\theta;\\mathbb{Z}^m|\\mathbb{Z})|\\mathbb{Z}, \\theta] - E[\\ell(\\theta;\\mathbb{Z}^m|\\mathbb{Z})|\\mathbb{Z}, \\theta]\\\\\n",
    "    = E_{\\text{Pr}(\\mathbb{Z}^m|\\mathbb{Z}, \\theta)}\\left(\\log \\frac{\\text{Pr}(\\mathbb{Z}^m|\\mathbb{Z}, \\theta')}{\\text{Pr}(\\mathbb{Z}^m|\\mathbb{Z}, \\theta)}\\right)\\\\\n",
    "    \\le0.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PROBLEM 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Ex. 8.3}$   \n",
    "Justify the estimate (8.50), using the relationship\n",
    "\\begin{equation}\n",
    "\\text{Pr}(A) = \\int \\text{Pr}(A|B)d(\\text{Pr}(B)).\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the relationship above, we have\n",
    "\n",
    "\\begin{equation}\n",
    "    \\widehat{\\text{Pr}}_{U_k}(u) = \\int \\text{Pr}_{U_k|U_l: l\\neq k}(u)d\\text{Pr}_{U_l: l \\neq k}.\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "The integral is thus estimated by a law-of-large-number way to be\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{1}{M-m+1}\\sum_{t=m}^M\\text{Pr}\\left(u|U_l^{(t)}, l\\neq k\\right).\n",
    "\\end{equation}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PROBLEM 3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$\\textbf{Ex. 8.4}$  \n",
    "Consider the bagging method of Section 8.7. Let our estimate $\\hat{f}(x)$ be the B-spline smoother $\\hat{\\mu}(x)$ of Section 8.2.1. Consider the parametric bootstrap of equation (8.6), applied to this estimator. Show that if we bag $\\hat{f}(x)$, using the parametric bootstrap to generate the bootstrap samples, the bagging estimate $\\hat{f}_{\\text{bag}}(x)$ converges to the original estimate $\\hat{f}(x)$ as $B \\rightarrow \\infty$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By definition of bagging we get\n",
    "\n",
    "\\begin{equation}\n",
    "\\label{eq:ex8-4bag}\n",
    "    \\hat f_{\\text{bag}}(x) = \\frac{1}{B}\\sum_{b=1}^B\\hat f^\\ast_b(x)\n",
    "\\end{equation}\n",
    "\n",
    "where\n",
    "\n",
    "\\begin{equation}\n",
    "    \\hat f^\\ast_b(x) = Sy^\\ast = S(\\hat f(x) + \\epsilon_b)\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "    S = N(N^TN)^{-1}N^T\n",
    "\\end{equation}\n",
    "\n",
    "and $\\epsilon_b\\sim N(0,\\sigma^2)$ for $B$-spline smoother. Note that $S^2 = S$, we obtain\n",
    "\n",
    "\\begin{equation}\n",
    "    \\hat f^\\ast_b(x) = S(\\hat f(x) + \\epsilon_b) = S(Sy+\\epsilon_b) = Sy + S\\epsilon_b.\n",
    "\\end{equation}\n",
    "\n",
    "Hence, it is reduced to\n",
    "\n",
    "\\begin{equation}\n",
    "    \\hat f_{\\text{bag}}(x) = Sy + S\\left(\\frac{1}{B}\\sum_{b=1}^B\\epsilon_b\\right).\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Now it is easy to realize that\n",
    "\n",
    "\\begin{equation}\n",
    "    \\lim_{B\\ra\\infty} \\hat f_{\\text{bag}}(x) = Sy = \\hat f(x). \n",
    "\\end{equation}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PROBLEM 4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Ex. 11.2}$  \n",
    "Consider a neural network for a quantitative outcome as in (11.5), using squared-error loss and identity output function $g_k(t) = t$. Suppose that the weights $\\alpha_m$ from the input to hidden layer are nearly zero. Show that the resulting model is nearly linear in the inputs."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One implicit assumption of this exercise is that the neural network uses sigmoid activation function (actually, any general activation function with the property that it's almost linear near origin). Recall that the sigmoid activation function is\n",
    "\n",
    "\\begin{equation}\n",
    "    \\sigma(\\nu) = \\frac{1}{1 + e^{-\\nu}}.\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Let $\\mu(x) = \\frac{1}{2}(1+x)$. Note that\n",
    "\n",
    "\\begin{equation}\n",
    "    \\lim_{x\\ra 0}\\mu(x)/\\sigma(x) = \\lim_{x\\ra 0}\\frac{1}{2}(1+x)(1+e^{-x})\\\\\n",
    "    = \\lim_{x\\ra 0}\\frac{1}{2}\\left(1 + e^{-x} + x + xe^{-x}\\right)\\\\\n",
    "    = 1.\n",
    "\\end{equation}\n",
    "\n",
    "When $\\alpha_m$ are nearly zero, so are $\\alpha_m^TX$. Note that if we add bias term into and enlarge $x$ (that is, each row of $x$ has 1 in the first position), then $Z_m = \\sigma(\\alpha_m^TX) \\sim \\frac{1}{2}(1 + \\alpha_m^TX)$. Therefore the resulting model is nearly linear in $x$, since $g_k$ is identity."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PROBLEM 5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Ex. 11.3}$   \n",
    "Derive the forward and backward propagation equations for the cross-entropy loss function.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For cross-entropy (deviance) we have\n",
    "\n",
    "\\begin{equation}\n",
    "\\label{eq:11-3a}\n",
    "    R(\\theta) = -\\sum_{i=1}^N\\sum_{k=1}^Ky_{ik}\\log(f_k(x_i)),\n",
    "\\end{equation}\n",
    "\n",
    "and the corresponding classifier is $G(x) = \\underset{k}{\\operatorname{argmax}}f_k(x)$.\n",
    "\n",
    "Let $z_{mi} = \\sigma(\\alpha_{0m} + \\alpha_m^Tx_i)$ from (11.5) in the text. Let $z_i = (z_{1i}, z_{2i}, ..., z_{Mi})$\n",
    "\n",
    "Then we have\n",
    "\n",
    "\\begin{equation}\n",
    "    R(\\theta) = \\sum_{i=1}^NR_i\\\\\n",
    "    = \\sum_{i=1}^N\\sum_{k=1}^K\\left(-y_{ik}\\log(f_k(x_i))\\right),  \n",
    "\\end{equation}\n",
    "\n",
    "With derivatives\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial R_i}{\\partial \\beta_{km}} = -\\frac{y_{ik}}{f_k(x_i)}g'_k(\\beta_k^Tz_i)z_{mi},\\\\\n",
    "    \\frac{\\partial R_i}{\\partial \\alpha_{ml}} = -\\sum_{k=1}^K\\frac{y_{ik}}{f_k(x_i)}g'_k(\\beta_k^Tz_i)\\beta_{km}\\sigma'(\\alpha_{0m} + \\alpha_m^Tx_i)x_{il}.\\label{eq:11-3b}\n",
    "\\end{equation}\n",
    "\n",
    "Given these derivatives, a gradient descent update at the $(r+1)$ st iteration has the form\n",
    "\n",
    "\\begin{equation}\n",
    "    \\beta_{km}^{(r+1)} = \\beta_{km}^{(r)} - \\gamma_r\\sum_{i=1}^N\\frac{\\partial R_i}{\\partial \\beta^{(r)}_{km}},\\\\\n",
    "    \\alpha_{ml}^{(r+1)} = \\alpha_{ml}^{(r)} - \\gamma_r\\sum_{i=1}^N\\frac{\\partial R_i}{\\partial \\alpha^{(r)}_{ml}},\n",
    "\\end{equation}\n",
    "\n",
    "where $\\gamma_r$ is the learning rate.  \n",
    "\n",
    "We write  \n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial R_i}{\\partial \\beta_{km}} = \\delta_{ki}z_{mi},\\\\\n",
    "    \\frac{\\partial R_i}{\\partial \\alpha_{ml}} = s_{mi}x_{il}.\n",
    "\\end{equation}\n",
    "\n",
    "From their definitions, we have\n",
    "\n",
    "\\begin{equation}\n",
    "    s_{mi} = \\sigma'(\\alpha_{0m}+\\alpha_m^Tx_i)\\sum_{k=1}^K\\beta_{km}\\delta_{ki},\n",
    "\\end{equation}\n",
    "\n",
    "known as the back-propagation equations."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PROBLEM 6"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a program to implement the neural network model of Net-2 in figure 11.10, and fit the neural network to the zipcode data. Compare the results to logistic regression on classification performance. The zipcode data are provided. Due to the large size of the dataset, you are only considering the classification of three digits, 0, 1, and 2. Use a 70-30 split for training and testing data.  \n",
    "\n",
    "Using library functions is acceptable"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Net-1: No hidden layer, equivalent to multinomial logistic regression. From an 16x16 input, to 10 outputs.\n",
    "Net-2: One hidden layer, 12 hidden units fully connected. From an 16x16 input, to 10 outputs, going through a hidden layer of 12 units.\n",
    "\n",
    "Write a program to implement the neural network model of Net-1 and Net-2 in figure 11.10, and fit the neural network to the zipcode data. Compare the results to logistic regression on classification performance. The zipcode data are provided. Due to the large size of the dataset, you are only considering the classification of three digits, 0, 1, and 2. Use a 70-30 split for training and testing data.  \n",
    "\n",
    "Using library functions is acceptable"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEPS TO BE FOLLOWED\n",
    "\n",
    "- Install the libraries and dependencies: tensorflow, numpy, sklearn\n",
    "- Then, create a Python script and import the required libraries\n",
    "- Load and preprocess the dataset\n",
    "- Create the Neural Network models for Net-1 and Net-2\n",
    "- Fit the Neural Network models to the training data\n",
    "- Evaluate the Neural Network models on the test data\n",
    "- Fit Logistic Regression to the trainig data\n",
    "- Evaluate the Logistic Regression model on the test data\n",
    "- Compare the models and the plots\n",
    "- Draw some conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape for df_0:  (1194, 256)\n",
      "Shape for df_1:  (1005, 256)\n",
      "Shape for df_2:  (731, 256)\n",
      "/n\n",
      "   0    1    2    3    4      5      6      7      8      9    ...    246   \n",
      "0 -1.0 -1.0 -1.0 -1.0 -1.0 -0.454  0.879 -0.745 -1.000 -1.000  ...  1.000  \\\n",
      "1 -1.0 -1.0 -1.0 -1.0 -1.0 -0.877  0.233  1.000  0.996  0.116  ... -0.041   \n",
      "2 -1.0 -1.0 -1.0 -1.0 -1.0 -0.990  0.019  0.640 -0.553 -0.999  ...  0.363   \n",
      "3 -1.0 -1.0 -1.0 -1.0 -1.0  0.100  0.452 -0.665 -0.358 -0.209  ...  0.865   \n",
      "4 -1.0 -1.0 -1.0 -1.0 -1.0 -1.000 -0.320  0.731 -0.657 -1.000  ... -0.663   \n",
      "\n",
      "     247    248    249    250    251  252  253  254  255  \n",
      "0  1.000  1.000  0.506 -0.174 -0.811 -1.0 -1.0 -1.0   -1  \n",
      "1  0.947  0.988  0.009 -0.931 -1.000 -1.0 -1.0 -1.0   -1  \n",
      "2  1.000  0.820 -0.064 -0.969 -1.000 -1.0 -1.0 -1.0   -1  \n",
      "3  1.000  1.000  0.838  0.085 -0.847 -1.0 -1.0 -1.0   -1  \n",
      "4  0.573  0.702 -0.762 -1.000 -1.000 -1.0 -1.0 -1.0   -1  \n",
      "\n",
      "[5 rows x 256 columns]\n",
      "/n\n",
      "DF_0\n",
      "/n\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1194 entries, 0 to 1193\n",
      "Columns: 256 entries, 0 to 255\n",
      "dtypes: float64(254), int64(2)\n",
      "memory usage: 2.3 MB\n",
      "None\n",
      "DF_1\n",
      "/n\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1005 entries, 0 to 1004\n",
      "Columns: 256 entries, 0 to 255\n",
      "dtypes: float64(154), int64(102)\n",
      "memory usage: 2.0 MB\n",
      "None\n",
      "DF_2\n",
      "/n\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 731 entries, 0 to 730\n",
      "Columns: 256 entries, 0 to 255\n",
      "dtypes: float64(254), int64(2)\n",
      "memory usage: 1.4 MB\n",
      "None\n",
      "/n\n"
     ]
    }
   ],
   "source": [
    "path_0 = 'https://raw.githubusercontent.com/ferriitoo/csv/main/digit0.txt'\n",
    "path_1 = 'https://raw.githubusercontent.com/ferriitoo/csv/main/digit1.txt'\n",
    "path_2 = 'https://raw.githubusercontent.com/ferriitoo/csv/main/digit2.txt'\n",
    "\n",
    "df_0 = pd.read_csv(path_0, header = None)\n",
    "df_1 = pd.read_csv(path_1, header = None)\n",
    "df_2 = pd.read_csv(path_2, header = None)\n",
    "\n",
    "print('Shape for df_0: ', df_0.shape)\n",
    "print('Shape for df_1: ', df_1.shape)\n",
    "print('Shape for df_2: ', df_2.shape)\n",
    "\n",
    "print('/n')\n",
    "print(df_0.head())\n",
    "print('/n')\n",
    "\n",
    "print('DF_0')\n",
    "print('/n')\n",
    "print(df_0.info())\n",
    "\n",
    "print('DF_1')\n",
    "print('/n')\n",
    "print(df_1.info())\n",
    "\n",
    "print('DF_2')\n",
    "print('/n')\n",
    "print(df_2.info())\n",
    "print('/n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel_selection\u001b[39;00m \u001b[39mimport\u001b[39;00m train_test_split\n\u001b[0;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlinear_model\u001b[39;00m \u001b[39mimport\u001b[39;00m LogisticRegression\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "def load_data():\n",
    "    path_0 = 'https://raw.githubusercontent.com/ferriitoo/csv/main/digit0.txt'\n",
    "    path_1 = 'https://raw.githubusercontent.com/ferriitoo/csv/main/digit1.txt'\n",
    "    path_2 = 'https://raw.githubusercontent.com/ferriitoo/csv/main/digit2.txt'\n",
    "\n",
    "    df_0 = pd.read_csv(path_0, header=None)\n",
    "    df_1 = pd.read_csv(path_1, header=None)\n",
    "    df_2 = pd.read_csv(path_2, header=None)\n",
    "\n",
    "    X = pd.concat([df_0, df_1, df_2])\n",
    "    y = np.concatenate([np.zeros(len(df_0)), np.ones(len(df_1)), np.full(len(df_2), 2)])\n",
    "\n",
    "    X = X.values / 255.0  # Normalize the input data\n",
    "    return X, y\n",
    "\n",
    "def preprocess_data(X, y):\n",
    "    X = X.reshape(-1, 16, 16, 1)  # Reshape the input data to a 16x16 format\n",
    "    y = tf.keras.utils.to_categorical(y, num_classes=3)  # One-hot encode the labels\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "X, y = load_data()\n",
    "X_train, X_test, y_train, y_test = preprocess_data(X, y)\n",
    "\n",
    "# Create the neural network models for Net-1 and Net-2\n",
    "def create_net1():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(16, 16, 1)),\n",
    "        tf.keras.layers.Dense(3, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def create_net2():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(16, 16, 1)),\n",
    "        tf.keras.layers.Dense(12, activation='relu'),\n",
    "        tf.keras.layers.Dense(3, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Fit the neural network models to the training data\n",
    "net1 = create_net1()\n",
    "net1.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.1)\n",
    "\n",
    "net2 = create_net2()\n",
    "net2.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Evaluate the neural network models on the test data\n",
    "net1_score = net1.evaluate(X_test, y_test)\n",
    "print(\"Net-1 Test Accuracy: \", net1_score[1])\n",
    "\n",
    "net2_score = net2.evaluate(X_test, y_test)\n",
    "print(\"Net-2 Test Accuracy: \", net2_score[1])\n",
    "\n",
    "# Fit logistic regression to the training data and evaluate on the test data\n",
    "logistic_regression = LogisticRegression()\n",
    "logistic_regression.fit(X_train.reshape(-1, 256), np.argmax(y_train, axis=1))\n",
    "y_pred = logistic_regression.predict(X_test.reshape(-1, 256))\n",
    "\n",
    "print(\"Logistic Regression Test Accuracy: \", logistic_regression.score(X_test.reshape(-1, 256), np.argmax(y_test, axis=1)))\n",
    "\n",
    "# Print the classification report\n",
    "print(\"Classification report for Logistic Regression:\")\n",
    "print(classification_report(np.argmax(y_test, axis=1), y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
