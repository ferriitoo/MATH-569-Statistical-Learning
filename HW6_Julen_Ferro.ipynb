{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.1: SOLUS LIBRO\n",
    "8.3: NO SOLUS LIBRO\n",
    "8.4: SOLUS LIBRO\n",
    "11.2: SOLUS LIBRO\n",
    "11.3: NO SOLUS LIBRO\n",
    "\n",
    "ORAIN KOPIATUKO DUT ESL_SOLUZIOETATIK"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PROBLEM 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**$\\textbf{Ex. 8.1}$**    \n",
    "**Let $r(y)$ and $q(y)$ be probability density functions. Jensen's inequality states that for a random variable $X$ and a convex function $\\phi(x)$, $\\mathbb{E}[\\phi(X)] \\geq \\phi[\\mathbb{E}(X)]$. Use Jensen's inequality to show that**\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbb{E}_q \\log \\frac{r(Y)}{q(Y)} \\tag{8.61}\n",
    "\\end{equation}\n",
    "\n",
    "**is maximized as a function of $r(y)$ when $r(y) = q(y)$. Hence show that $R(\\theta, \\theta) \\geq R(\\theta', \\theta)$ as stated below equation (8.46).**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that $-\\log(x)$ is convex, by Jensen's inequality, we have  \n",
    "\n",
    "\\begin{equation}\n",
    "    E_q[-\\log[r(Y)/q(Y)]] \\ge -\\log[E_q[r(Y)/q(Y)]]\\\\\n",
    "    = -\\log\\left[\\int\\frac{r(y)}{q(y)}q(y)dy\\right]\\\\\n",
    "    = -\\log\\left[\\int r(y)dy\\right]\\\\\n",
    "    = -\\log(1)\\\\\n",
    "    = 0,\n",
    "\\end{equation}\n",
    "\n",
    "therefore we have\n",
    "\n",
    "\\begin{equation}\n",
    "    E_q[\\log(r(Y)/q(Y))]\\le 0 = E_q[\\log(q(Y)/q(Y))].\n",
    "\\end{equation}\n",
    "\n",
    "So the expectation is maximized when $r = q$  \n",
    "\n",
    "\n",
    "For equation (8.46) in the text, we have\n",
    "\n",
    "\\begin{equation}\n",
    "    R(\\theta', \\theta) - R(\\theta, \\theta) = E[\\ell_1(\\theta;\\mathbb{Z}^m|\\mathbb{Z})|\\mathbb{Z}, \\theta] - E[\\ell(\\theta;\\mathbb{Z}^m|\\mathbb{Z})|\\mathbb{Z}, \\theta]\\\\\n",
    "    = E_{\\text{Pr}(\\mathbb{Z}^m|\\mathbb{Z}, \\theta)}\\left(\\log \\frac{\\text{Pr}(\\mathbb{Z}^m|\\mathbb{Z}, \\theta')}{\\text{Pr}(\\mathbb{Z}^m|\\mathbb{Z}, \\theta)}\\right)\\\\\n",
    "    \\le0.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PROBLEM 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Ex. 8.3}$   \n",
    "Justify the estimate (8.50), using the relationship\n",
    "\\begin{equation}\n",
    "\\text{Pr}(A) = \\int \\text{Pr}(A|B)d(\\text{Pr}(B)).\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the relationship above, we have\n",
    "\n",
    "\\begin{equation}\n",
    "    \\widehat{\\text{Pr}}_{U_k}(u) = \\int \\text{Pr}_{U_k|U_l: l\\neq k}(u)d\\text{Pr}_{U_l: l \\neq k}.\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "The integral is thus estimated by a law-of-large-number way to be\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{1}{M-m+1}\\sum_{t=m}^M\\text{Pr}\\left(u|U_l^{(t)}, l\\neq k\\right).\n",
    "\\end{equation}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PROBLEM 3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$\\textbf{Ex. 8.4}$  \n",
    "Consider the bagging method of Section 8.7. Let our estimate $\\hat{f}(x)$ be the B-spline smoother $\\hat{\\mu}(x)$ of Section 8.2.1. Consider the parametric bootstrap of equation (8.6), applied to this estimator. Show that if we bag $\\hat{f}(x)$, using the parametric bootstrap to generate the bootstrap samples, the bagging estimate $\\hat{f}_{\\text{bag}}(x)$ converges to the original estimate $\\hat{f}(x)$ as $B \\rightarrow \\infty$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By definition of bagging we get\n",
    "\n",
    "\\begin{equation}\n",
    "\\label{eq:ex8-4bag}\n",
    "    \\hat f_{\\text{bag}}(x) = \\frac{1}{B}\\sum_{b=1}^B\\hat f^\\ast_b(x)\n",
    "\\end{equation}\n",
    "\n",
    "where\n",
    "\n",
    "\\begin{equation}\n",
    "    \\hat f^\\ast_b(x) = Sy^\\ast = S(\\hat f(x) + \\epsilon_b)\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "    S = N(N^TN)^{-1}N^T\n",
    "\\end{equation}\n",
    "\n",
    "and $\\epsilon_b\\sim N(0,\\sigma^2)$ for $B$-spline smoother. Note that $S^2 = S$, we obtain\n",
    "\n",
    "\\begin{equation}\n",
    "    \\hat f^\\ast_b(x) = S(\\hat f(x) + \\epsilon_b) = S(Sy+\\epsilon_b) = Sy + S\\epsilon_b.\n",
    "\\end{equation}\n",
    "\n",
    "Hence, it is reduced to\n",
    "\n",
    "\\begin{equation}\n",
    "    \\hat f_{\\text{bag}}(x) = Sy + S\\left(\\frac{1}{B}\\sum_{b=1}^B\\epsilon_b\\right).\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Now it is easy to realize that\n",
    "\n",
    "\\begin{equation}\n",
    "    \\lim_{B\\ra\\infty} \\hat f_{\\text{bag}}(x) = Sy = \\hat f(x). \n",
    "\\end{equation}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PROBLEM 4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Ex. 11.2}$  \n",
    "Consider a neural network for a quantitative outcome as in (11.5), using squared-error loss and identity output function $g_k(t) = t$. Suppose that the weights $\\alpha_m$ from the input to hidden layer are nearly zero. Show that the resulting model is nearly linear in the inputs."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One implicit assumption of this exercise is that the neural network uses sigmoid activation function (actually, any general activation function with the property that it's almost linear near origin). Recall that the sigmoid activation function is\n",
    "\n",
    "\\begin{equation}\n",
    "    \\sigma(\\nu) = \\frac{1}{1 + e^{-\\nu}}.\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Let $\\mu(x) = \\frac{1}{2}(1+x)$. Note that\n",
    "\n",
    "\\begin{equation}\n",
    "    \\lim_{x\\ra 0}\\mu(x)/\\sigma(x) = \\lim_{x\\ra 0}\\frac{1}{2}(1+x)(1+e^{-x})\\\\\n",
    "    = \\lim_{x\\ra 0}\\frac{1}{2}\\left(1 + e^{-x} + x + xe^{-x}\\right)\\\\\n",
    "    = 1.\n",
    "\\end{equation}\n",
    "\n",
    "When $\\alpha_m$ are nearly zero, so are $\\alpha_m^TX$. Note that if we add bias term into and enlarge $x$ (that is, each row of $x$ has 1 in the first position), then $Z_m = \\sigma(\\alpha_m^TX) \\sim \\frac{1}{2}(1 + \\alpha_m^TX)$. Therefore the resulting model is nearly linear in $x$, since $g_k$ is identity."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PROBLEM 5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Ex. 11.3}$   \n",
    "Derive the forward and backward propagation equations for the cross-entropy loss function.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For cross-entropy (deviance) we have\n",
    "\n",
    "\\begin{equation}\n",
    "\\label{eq:11-3a}\n",
    "    R(\\theta) = -\\sum_{i=1}^N\\sum_{k=1}^Ky_{ik}\\log(f_k(x_i)),\n",
    "\\end{equation}\n",
    "\n",
    "and the corresponding classifier is $G(x) = \\underset{k}{\\operatorname{argmax}}f_k(x)$.\n",
    "\n",
    "Let $z_{mi} = \\sigma(\\alpha_{0m} + \\alpha_m^Tx_i)$ from (11.5) in the text. Let $z_i = (z_{1i}, z_{2i}, ..., z_{Mi})$\n",
    "\n",
    "Then we have\n",
    "\n",
    "\\begin{equation}\n",
    "    R(\\theta) = \\sum_{i=1}^NR_i\\\\\n",
    "    = \\sum_{i=1}^N\\sum_{k=1}^K\\left(-y_{ik}\\log(f_k(x_i))\\right),  \n",
    "\\end{equation}\n",
    "\n",
    "With derivatives\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial R_i}{\\partial \\beta_{km}} = -\\frac{y_{ik}}{f_k(x_i)}g'_k(\\beta_k^Tz_i)z_{mi},\\\\\n",
    "    \\frac{\\partial R_i}{\\partial \\alpha_{ml}} = -\\sum_{k=1}^K\\frac{y_{ik}}{f_k(x_i)}g'_k(\\beta_k^Tz_i)\\beta_{km}\\sigma'(\\alpha_{0m} + \\alpha_m^Tx_i)x_{il}.\\label{eq:11-3b}\n",
    "\\end{equation}\n",
    "\n",
    "Given these derivatives, a gradient descent update at the $(r+1)$ st iteration has the form\n",
    "\n",
    "\\begin{equation}\n",
    "    \\beta_{km}^{(r+1)} = \\beta_{km}^{(r)} - \\gamma_r\\sum_{i=1}^N\\frac{\\partial R_i}{\\partial \\beta^{(r)}_{km}},\\\\\n",
    "    \\alpha_{ml}^{(r+1)} = \\alpha_{ml}^{(r)} - \\gamma_r\\sum_{i=1}^N\\frac{\\partial R_i}{\\partial \\alpha^{(r)}_{ml}},\n",
    "\\end{equation}\n",
    "\n",
    "where $\\gamma_r$ is the learning rate.  \n",
    "\n",
    "We write  \n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial R_i}{\\partial \\beta_{km}} = \\delta_{ki}z_{mi},\\\\\n",
    "    \\frac{\\partial R_i}{\\partial \\alpha_{ml}} = s_{mi}x_{il}.\n",
    "\\end{equation}\n",
    "\n",
    "From their definitions, we have\n",
    "\n",
    "\\begin{equation}\n",
    "    s_{mi} = \\sigma'(\\alpha_{0m}+\\alpha_m^Tx_i)\\sum_{k=1}^K\\beta_{km}\\delta_{ki},\n",
    "\\end{equation}\n",
    "\n",
    "known as the back-propagation equations."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PROBLEM 6"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a program to implement the neural network model of Net-2 in figure 11.10, and fit the neural network to the zipcode data. Compare the results to\n",
    "logistic regression on classification performance. The zipcode data are provided. Due to the large size of the dataset, you are only considering the classification of three digits, 0, 1, and 2. Use a 70-30 split for training and testing data.  \n",
    "\n",
    "Using library functions is acceptable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF_0\n",
      "/n\n",
      "                -1         -1.1         -1.2         -1.3         -1.4   \n",
      "count  1193.000000  1193.000000  1193.000000  1193.000000  1193.000000  \\\n",
      "mean     -0.998627    -0.995394    -0.984910    -0.941202    -0.833203   \n",
      "std       0.047424     0.081349     0.140083     0.251006     0.425025   \n",
      "min      -1.000000    -1.000000    -1.000000    -1.000000    -1.000000   \n",
      "25%      -1.000000    -1.000000    -1.000000    -1.000000    -1.000000   \n",
      "50%      -1.000000    -1.000000    -1.000000    -1.000000    -1.000000   \n",
      "75%      -1.000000    -1.000000    -1.000000    -1.000000    -1.000000   \n",
      "max       0.638000     0.737000     1.000000     1.000000     1.000000   \n",
      "\n",
      "            -0.454        0.879       -0.745         -1.5         -1.6  ...   \n",
      "count  1193.000000  1193.000000  1193.000000  1193.000000  1193.000000  ...  \\\n",
      "mean     -0.571521    -0.132432     0.153360     0.047161    -0.353159  ...   \n",
      "std       0.610513     0.729190     0.733777     0.750074     0.676785  ...   \n",
      "min      -1.000000    -1.000000    -1.000000    -1.000000    -1.000000  ...   \n",
      "25%      -1.000000    -0.939000    -0.567000    -0.780000    -1.000000  ...   \n",
      "50%      -0.958000    -0.162000     0.335000     0.162000    -0.567000  ...   \n",
      "75%      -0.258000     0.550000     0.825000     0.764000     0.201000  ...   \n",
      "max       1.000000     1.000000     1.000000     1.000000     1.000000  ...   \n",
      "\n",
      "              1.17         1.18         1.19        0.506       -0.174   \n",
      "count  1193.000000  1193.000000  1193.000000  1193.000000  1193.000000  \\\n",
      "mean      0.282427     0.527630     0.438582     0.042100    -0.467260   \n",
      "std       0.662562     0.637476     0.648822     0.668933     0.583756   \n",
      "min      -1.000000    -1.000000    -1.000000    -1.000000    -1.000000   \n",
      "25%      -0.152000     0.298000     0.171000    -0.545000    -1.000000   \n",
      "50%       0.448000     0.828000     0.677000     0.107000    -0.679000   \n",
      "75%       0.875000     1.000000     1.000000     0.588000    -0.063000   \n",
      "max       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
      "\n",
      "            -0.811       -1.126       -1.127       -1.128  -1.129  \n",
      "count  1193.000000  1193.000000  1193.000000  1193.000000  1193.0  \n",
      "mean     -0.825518    -0.967484    -0.996522    -0.999936    -1.0  \n",
      "std       0.361104     0.157981     0.042196     0.002061     0.0  \n",
      "min      -1.000000    -1.000000    -1.000000    -1.000000    -1.0  \n",
      "25%      -1.000000    -1.000000    -1.000000    -1.000000    -1.0  \n",
      "50%      -1.000000    -1.000000    -1.000000    -1.000000    -1.0  \n",
      "75%      -0.841000    -1.000000    -1.000000    -1.000000    -1.0  \n",
      "max       1.000000     1.000000     0.233000    -0.929000    -1.0  \n",
      "\n",
      "[8 rows x 256 columns]\n",
      "DF_1\n",
      "/n\n",
      "           -1    -1.1    -1.2    -1.3         -1.4         -1.5         -1.6   \n",
      "count  1004.0  1004.0  1004.0  1004.0  1004.000000  1004.000000  1004.000000  \\\n",
      "mean     -1.0    -1.0    -1.0    -1.0    -0.999996    -0.987265    -0.634793   \n",
      "std       0.0     0.0     0.0     0.0     0.000126     0.086569     0.502570   \n",
      "min      -1.0    -1.0    -1.0    -1.0    -1.000000    -1.000000    -1.000000   \n",
      "25%      -1.0    -1.0    -1.0    -1.0    -1.000000    -1.000000    -1.000000   \n",
      "50%      -1.0    -1.0    -1.0    -1.0    -1.000000    -1.000000    -0.890000   \n",
      "75%      -1.0    -1.0    -1.0    -1.0    -1.000000    -1.000000    -0.418500   \n",
      "max      -1.0    -1.0    -1.0    -1.0    -0.996000     0.412000     1.000000   \n",
      "\n",
      "              0.51       -0.213         -1.7  ...       -0.654        0.666   \n",
      "count  1004.000000  1004.000000  1004.000000  ...  1004.000000  1004.000000  \\\n",
      "mean      0.477215    -0.000085    -0.881551  ...    -0.839163     0.150311   \n",
      "std       0.462180     0.583939     0.278868  ...     0.324090     0.546625   \n",
      "min      -1.000000    -1.000000    -1.000000  ...    -1.000000    -1.000000   \n",
      "25%       0.228500    -0.487000    -1.000000  ...    -1.000000    -0.253000   \n",
      "50%       0.627000    -0.023000    -1.000000  ...    -0.992000     0.216000   \n",
      "75%       0.823250     0.509250    -0.915000  ...    -0.826000     0.598500   \n",
      "max       1.000000     1.000000     0.999000  ...     1.000000     1.000000   \n",
      "\n",
      "             0.301       -1.198       -1.199       -1.200       -1.201   \n",
      "count  1004.000000  1004.000000  1004.000000  1004.000000  1004.000000  \\\n",
      "mean      0.509477    -0.567092    -0.960054    -0.996259    -0.999337   \n",
      "std       0.464532     0.566660     0.203505     0.064148     0.021019   \n",
      "min      -1.000000    -1.000000    -1.000000    -1.000000    -1.000000   \n",
      "25%       0.242500    -1.000000    -1.000000    -1.000000    -1.000000   \n",
      "50%       0.661000    -0.853000    -1.000000    -1.000000    -1.000000   \n",
      "75%       0.883250    -0.310000    -1.000000    -1.000000    -1.000000   \n",
      "max       1.000000     1.000000     0.999000     0.502000    -0.334000   \n",
      "\n",
      "            -1.202  -1.203  -1.204  \n",
      "count  1004.000000  1004.0  1004.0  \n",
      "mean     -0.999532    -1.0    -1.0  \n",
      "std       0.014833     0.0     0.0  \n",
      "min      -1.000000    -1.0    -1.0  \n",
      "25%      -1.000000    -1.0    -1.0  \n",
      "50%      -1.000000    -1.0    -1.0  \n",
      "75%      -1.000000    -1.0    -1.0  \n",
      "max      -0.530000    -1.0    -1.0  \n",
      "\n",
      "[8 rows x 256 columns]\n",
      "DF_2\n",
      "/n\n",
      "               -1        -1.1        -1.2        -1.3        -1.4      -0.798   \n",
      "count  730.000000  730.000000  730.000000  730.000000  730.000000  730.000000  \\\n",
      "mean    -0.992478   -0.960337   -0.902442   -0.799071   -0.602919   -0.373090   \n",
      "std      0.071395    0.216003    0.346138    0.465489    0.619308    0.737769   \n",
      "min     -1.000000   -1.000000   -1.000000   -1.000000   -1.000000   -1.000000   \n",
      "25%     -1.000000   -1.000000   -1.000000   -1.000000   -1.000000   -1.000000   \n",
      "50%     -1.000000   -1.000000   -1.000000   -1.000000   -1.000000   -0.817000   \n",
      "75%     -1.000000   -1.000000   -1.000000   -0.961750   -0.306250    0.278500   \n",
      "max      0.166000    0.999000    1.000000    1.000000    1.000000    1.000000   \n",
      "\n",
      "              0.3       0.432      -0.799        -1.5  ...      -0.947   \n",
      "count  730.000000  730.000000  730.000000  730.000000  ...  730.000000  \\\n",
      "mean    -0.196452   -0.153579   -0.319710   -0.623868  ...   -0.552314   \n",
      "std      0.781372    0.812740    0.765821    0.607593  ...    0.688769   \n",
      "min     -1.000000   -1.000000   -1.000000   -1.000000  ...   -1.000000   \n",
      "25%     -1.000000   -1.000000   -1.000000   -1.000000  ...   -1.000000   \n",
      "50%     -0.333000   -0.301500   -0.769000   -1.000000  ...   -1.000000   \n",
      "75%      0.580500    0.685500    0.447000   -0.426250  ...   -0.053000   \n",
      "max      1.000000    1.000000    1.000000    1.000000  ...    1.000000   \n",
      "\n",
      "           -0.524       0.307        0.39       0.852       0.751      0.99.1   \n",
      "count  730.000000  730.000000  730.000000  730.000000  730.000000  730.000000  \\\n",
      "mean    -0.681451   -0.783026   -0.838108   -0.844815   -0.797688   -0.720532   \n",
      "std      0.597715    0.497864    0.430942    0.438016    0.476977    0.557233   \n",
      "min     -1.000000   -1.000000   -1.000000   -1.000000   -1.000000   -1.000000   \n",
      "25%     -1.000000   -1.000000   -1.000000   -1.000000   -1.000000   -1.000000   \n",
      "50%     -1.000000   -1.000000   -1.000000   -1.000000   -1.000000   -1.000000   \n",
      "75%     -0.722750   -1.000000   -1.000000   -1.000000   -1.000000   -0.814500   \n",
      "max      1.000000    1.000000    1.000000    1.000000    1.000000    1.000000   \n",
      "\n",
      "            0.567      -0.664      -1.132  \n",
      "count  730.000000  730.000000  730.000000  \n",
      "mean    -0.724952   -0.825463   -0.948782  \n",
      "std      0.557129    0.444196    0.201360  \n",
      "min     -1.000000   -1.000000   -1.000000  \n",
      "25%     -1.000000   -1.000000   -1.000000  \n",
      "50%     -1.000000   -1.000000   -1.000000  \n",
      "75%     -0.851750   -1.000000   -1.000000  \n",
      "max      1.000000    1.000000    0.592000  \n",
      "\n",
      "[8 rows x 256 columns]\n"
     ]
    }
   ],
   "source": [
    "path_0 = 'https://raw.githubusercontent.com/ferriitoo/csv/main/digit0.txt'\n",
    "path_1 = 'https://raw.githubusercontent.com/ferriitoo/csv/main/digit1.txt'\n",
    "path_2 = 'https://raw.githubusercontent.com/ferriitoo/csv/main/digit2.txt'\n",
    "\n",
    "df_0 = pd.read_csv(path_0)\n",
    "df_1 = pd.read_csv(path_1)\n",
    "df_2 = pd.read_csv(path_2)\n",
    "\n",
    "print('DF_0')\n",
    "print('/n')\n",
    "print(df_0.describe())\n",
    "\n",
    "print('DF_1')\n",
    "print('/n')\n",
    "print(df_1.describe())\n",
    "\n",
    "print('DF_2')\n",
    "print('/n')\n",
    "print(df_2.describe())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
