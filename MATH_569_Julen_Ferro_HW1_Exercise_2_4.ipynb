{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PROBLEM 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The median distance from the origin to the closest data point can be found by considering the cumulative distribution function (CDF) of the distance between the origin and a randomly selected data point. If we denote the distance between the origin and a randomly selected data point as r, the CDF of r is given by:\n",
    "\n",
    "For the p = 2 bi-dimensinal case\n",
    "\n",
    "F(r) = P(r < d) = (Area of the circle with radius r) / (Area of the unit ball) = (π * r^2) / (π * 1^2) = r^2\n",
    "\n",
    "Since we want to find the median distance, we set F(r) = 1/2 and solve for r:\n",
    "\n",
    "r^2 = 1/2\n",
    "r = sqrt(1/2) = 1/sqrt(2)\n",
    "\n",
    "Therefore, the median distance from the origin to the closest data point is approximately 0.71 (to 2 decimal places)\n",
    "\n",
    "0.71 * r is greater than 0.5 * r due to the fact that in th 2-D unit ball the data points concentration is much greater in the outer part of the circle than in the inner part of it.   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PROBLEM 4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Theoretically, the result of f(0) is = 1. If we perform the simulation, we get a value close to 0, which is not the correct one. Therefore, we can say that the bias error is high due to the fact that the final result is always  close to 0, not to 1.\n",
    "\n",
    "We know that the input data is homogeneously distributed in a p-dimensional unit ball space (origin- centered). Hence, the probability of any point to be closest to the origin is going to be the samefor all. Thus, the average value of the nearest neighbors will not be a good (accurate) estimator because it will not fit into the correct answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "import math\n",
    "import tensorflow as tf\n",
    "import random as rand\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "N = 200\n",
    "p = 10\n",
    "\n",
    "def f(x):\n",
    "    sum = 0\n",
    "    for h in range(p):\n",
    "        sum += pow(x[h], 2)\n",
    "    inside = pow(sum, 1/p)\n",
    "    result = math.exp((-10)*inside)\n",
    "    return result\n",
    "\n",
    "# Generate \"num_points\" random points in \"dimension\" that have uniform\n",
    "# probability over the unit ball scaled by \"radius\" (length of points\n",
    "# are in range [0, \"radius\"]).\n",
    "def random_ball(num_points, dimension, radius=1):\n",
    "    from numpy import random, linalg\n",
    "    # First generate random directions by normalizing the length of a\n",
    "    # vector of random-normal values (these distribute evenly on ball).\n",
    "    random_directions = random.normal(size=(dimension,num_points))\n",
    "    random_directions /= linalg.norm(random_directions, axis=0)\n",
    "    # Second generate a random radius with probability proportional to\n",
    "    # the surface area of a ball with a given radius.\n",
    "    random_radii = random.random(num_points) ** (1/dimension)\n",
    "    # Return the list of random (direction & length) points.\n",
    "    return radius * (random_directions * random_radii).T\n",
    "\n",
    "\n",
    "def plot(X_train):\n",
    "    plt.xlabel(\"X1\")\n",
    "    plt.ylabel(\"X2\")\n",
    "    plt.title(\"Homogeneously distributed p-ball sampling\")\n",
    "    plt.scatter(X_train.loc[:, 0], X_train.loc[:, 1], color = 'orange')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "X_radius = pd.DataFrame(random_ball(N, p))\n",
    "for i in range(N): X_radius['result'] = f(X_radius.loc[i, :9])\n",
    "\n",
    "# print(X_radius.head())    \n",
    "# plot(X_radius)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_y = X_radius.loc[:, 9]\n",
    "y_hat = result_y.mean()\n",
    "y_std = result_y.std()\n",
    "\n",
    "# print(result_y)\n",
    "# print(y_hat)\n",
    "# print(y_std)\n",
    "\n",
    "# plt.hist(result_y, bins = 50)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a Nearest Neighbor. k = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted result is:  4.9e-05\n",
      "The accurate result is:  1.0\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "  \n",
    "  \n",
    "# Create feature and target arrays\n",
    "X = X_radius.loc[:, :9]\n",
    "y = X_radius['result']\n",
    "  \n",
    "# Split into training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=42)\n",
    "  \n",
    "knn = KNeighborsRegressor(n_neighbors=1)\n",
    "  \n",
    "knn.fit(X_train, y_train)\n",
    "  \n",
    "# Predict on dataset which model has not seen before\n",
    "test = np.zeros((1,p))\n",
    "pred = round(knn.predict(test)[0], 6)\n",
    "#knn.score(X_train, y_train)\n",
    "\n",
    "correct = round(f(np.zeros(p)), 5)\n",
    "\n",
    "print(\"The predicted result is: \", pred )\n",
    "print(\"The accurate result is: \", correct)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look how 4.7e-05 is far away from 1.0. Thus, demonstrating the high bias of the estimator."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fec7c492cb63a891b61f6a129c254b2f9b08ab3c88ef1c1ffe13c4cf2a66800d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
