{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PROBLEM 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solution involves using the cumulative distribution function (CDF) of the minimum distance from the origin to n points drawn uniformly from a p-dimensional unit ball centered at the origin. The CDF can be derived by finding the probability that the minimum distance is less than a certain value y, which can be calculated as the volume of the p-dimensional unit ball centered at the origin that is inside the sphere of radius y around the origin. The median of the CDF then corresponds to the value of y such that 50% of the points are closer to the origin than y and 50% are farther away.\n",
    "\n",
    "To find the CDF, we first need to find the probability density function (PDF) of the distance from the origin to a single point xi. The PDF is given by the product of the volume of the unit ball and the reciprocal of the total number of points, which is proportional to the volume of the p-dimensional sphere of radius y.\n",
    "\n",
    "Next, we use the CDF of the minimum of n independent and identically distributed random variables to find the CDF of the minimum distance from the origin to the closest data point. The CDF can be expressed as the product of the CDF of the distance to a single point and the complementary cumulative distribution function (CCDF) of the maximum distance, raised to the power of n.\n",
    "\n",
    "Finally, we find the median of the CDF by setting it equal to 0.5 and solving for y. The median distance from the origin to the closest data point can then be calculated. The specific expression for the median distance will depend on the dimensionality of the unit ball, but in general it will increase as the number of data points n increases and as the dimensionality of the unit ball p increases.\n",
    "\n",
    "Taking into account the equations:\n",
    "\n",
    "The probability density function (PDF) of the distance from the origin to a single point xi can be expressed as:\n",
    "\n",
    "f(y) = n * Vp * (1/Vp) * (y^(p-1))/y^p\n",
    "where n is the number of points, Vp is the volume of the p-dimensional unit ball, and y is the distance from the origin to the closest point.\n",
    "\n",
    "The CDF of the minimum distance from the origin to the closest point can then be derived as:\n",
    "\n",
    "F(y) = (1 - (1 - f(y))^n)\n",
    "\n",
    "Finally, to find the median distance from the origin to the closest point, we set F(y) = 0.5 and solve for y:\n",
    "\n",
    "0.5 = 1 - (1 - f(y))^n\n",
    "1 - 0.5 = (1 - f(y))^n\n",
    "0.5 = (1 - y^p)^n\n",
    "(1 - 0.5^(1/n))^(1/p) = y\n",
    "\n",
    "\n",
    "d(p,N)  = \\left(1-\\frac{1}{2}^{1/N}\\right)^{1/p}.\n",
    "d(p,N)  = \\left(1-\\frac{1}{2}^{1/N}\\right)^{1/2}.\n",
    "\n",
    "The specific expression for the median distance will depend on the dimensionality of the unit ball and the number of data points. In general, the median distance will increase as the number of data points n increases and as the dimensionality of the unit ball p increases."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PROBLEM 4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Theoretically, the result of f(0) is = 1. If we perform the simulation, we get a value close to 0, which is not the correct one. Therefore, we can say that the bias error is high due to the fact that the final result is always  close to 0, not to 1.\n",
    "\n",
    "We know that the input data is homogeneously distributed in a p-dimensional unit ball space (origin- centered). Hence, the probability of any point to be closest to the origin is going to be the samefor all. Thus, the average value of the nearest neighbors will not be a good (accurate) estimator because it will not fit into the correct answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "import math\n",
    "import tensorflow as tf\n",
    "import random as rand\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "N = 200\n",
    "p = 10\n",
    "\n",
    "def f(x):\n",
    "    sum = 0\n",
    "    for h in range(p):\n",
    "        sum += pow(x[h], 2)\n",
    "    inside = pow(sum, 1/p)\n",
    "    result = math.exp((-10)*inside)\n",
    "    return result\n",
    "\n",
    "# Generate \"num_points\" random points in \"dimension\" that have uniform\n",
    "# probability over the unit ball scaled by \"radius\" (length of points\n",
    "# are in range [0, \"radius\"]).\n",
    "def random_ball(num_points, dimension, radius=1):\n",
    "    from numpy import random, linalg\n",
    "    # First generate random directions by normalizing the length of a\n",
    "    # vector of random-normal values (these distribute evenly on ball).\n",
    "    random_directions = random.normal(size=(dimension,num_points))\n",
    "    random_directions /= linalg.norm(random_directions, axis=0)\n",
    "    # Second generate a random radius with probability proportional to\n",
    "    # the surface area of a ball with a given radius.\n",
    "    random_radii = random.random(num_points) ** (1/dimension)\n",
    "    # Return the list of random (direction & length) points.\n",
    "    return radius * (random_directions * random_radii).T\n",
    "\n",
    "\n",
    "def plot(X_train):\n",
    "    plt.xlabel(\"X1\")\n",
    "    plt.ylabel(\"X2\")\n",
    "    plt.title(\"Homogeneously distributed p-ball sampling\")\n",
    "    plt.scatter(X_train.loc[:, 0], X_train.loc[:, 1], color = 'orange')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "X_radius = pd.DataFrame(random_ball(N, p))\n",
    "for i in range(N): X_radius['result'] = f(X_radius.loc[i, :9])\n",
    "\n",
    "# print(X_radius.head())    \n",
    "# plot(X_radius)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_y = X_radius.loc[:, 9]\n",
    "y_hat = result_y.mean()\n",
    "y_std = result_y.std()\n",
    "\n",
    "# print(result_y)\n",
    "# print(y_hat)\n",
    "# print(y_std)\n",
    "\n",
    "# plt.hist(result_y, bins = 50)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a Nearest Neighbor. k = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted result is:  4.8e-05\n",
      "The accurate result is:  1.0\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "  \n",
    "  \n",
    "# Create feature and target arrays\n",
    "X = X_radius.loc[:, :9]\n",
    "y = X_radius['result']\n",
    "  \n",
    "# Split into training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=42)\n",
    "  \n",
    "knn = KNeighborsRegressor(n_neighbors=1)\n",
    "  \n",
    "knn.fit(X_train, y_train)\n",
    "  \n",
    "# Predict on dataset which model has not seen before\n",
    "test = np.zeros((1,p))\n",
    "pred = round(knn.predict(test)[0], 6)\n",
    "#knn.score(X_train, y_train)\n",
    "\n",
    "correct = round(f(np.zeros(p)), 5)\n",
    "\n",
    "print(\"The predicted result is: \", pred )\n",
    "print(\"The accurate result is: \", correct)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look how 4.7e-05 is far away from 1.0. Thus, demonstrating the high bias of the estimator."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fec7c492cb63a891b61f6a129c254b2f9b08ab3c88ef1c1ffe13c4cf2a66800d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
